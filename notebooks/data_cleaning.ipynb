{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f9e9256",
   "metadata": {},
   "source": [
    "# Philadelphia 311 Data Cleaning Pipeline\n",
    "This notebook implements comprehensive data cleaning for the Philadelphia 311 Service Requests dataset including:\n",
    "- Missing value handling\n",
    "- Deduplication\n",
    "- Column standardization\n",
    "- Data type conversion\n",
    "- Location validation\n",
    "- Text normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "691d083b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227a2745",
   "metadata": {},
   "source": [
    "## Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb119a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: The raw data is available for download at:\n",
      "https://falconbgsu-my.sharepoint.com/:f:/g/personal/lmoraa_bgsu_edu/IgBxvcsA2OrmQaDBpFTlbfAZAeon55WcaDPhRLTOGjI4v6c?e=fxfd82\n",
      "Original dataset shape: (518841, 18)\n",
      "Memory usage: 445.54 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>objectid</th>\n",
       "      <th>service_request_id</th>\n",
       "      <th>subject</th>\n",
       "      <th>status</th>\n",
       "      <th>status_notes</th>\n",
       "      <th>service_name</th>\n",
       "      <th>service_code</th>\n",
       "      <th>agency_responsible</th>\n",
       "      <th>service_notice</th>\n",
       "      <th>requested_datetime</th>\n",
       "      <th>updated_datetime</th>\n",
       "      <th>expected_datetime</th>\n",
       "      <th>closed_datetime</th>\n",
       "      <th>address</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>media_url</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5056952</td>\n",
       "      <td>17346520</td>\n",
       "      <td>Graffiti Removal</td>\n",
       "      <td>Closed</td>\n",
       "      <td>Issue Resolved</td>\n",
       "      <td>Graffiti Removal</td>\n",
       "      <td>SR-CL01</td>\n",
       "      <td>Community Life Improvement Program</td>\n",
       "      <td>7 Business Days</td>\n",
       "      <td>2025-01-01 00:00:34+00</td>\n",
       "      <td>2025-01-07 12:53:27+00</td>\n",
       "      <td>2025-01-10 00:00:00+00</td>\n",
       "      <td>2025-01-07 12:53:24+00</td>\n",
       "      <td>1701 SPRING GARDEN ST</td>\n",
       "      <td>19130.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39.963169</td>\n",
       "      <td>-75.166457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5056953</td>\n",
       "      <td>17346521</td>\n",
       "      <td>Graffiti Removal</td>\n",
       "      <td>Closed</td>\n",
       "      <td>Issue Resolved</td>\n",
       "      <td>Graffiti Removal</td>\n",
       "      <td>SR-CL01</td>\n",
       "      <td>Community Life Improvement Program</td>\n",
       "      <td>7 Business Days</td>\n",
       "      <td>2025-01-01 00:03:51+00</td>\n",
       "      <td>2025-01-07 12:52:31+00</td>\n",
       "      <td>2025-01-10 00:00:00+00</td>\n",
       "      <td>2025-01-07 12:52:28+00</td>\n",
       "      <td>1901 SPRING GARDEN ST</td>\n",
       "      <td>19130.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39.963625</td>\n",
       "      <td>-75.169500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5056954</td>\n",
       "      <td>17346523</td>\n",
       "      <td>Recycling Collection</td>\n",
       "      <td>Closed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rubbish/Recyclable Material Collection</td>\n",
       "      <td>SR-ST03</td>\n",
       "      <td>Streets Department</td>\n",
       "      <td>2 Business Days</td>\n",
       "      <td>2025-01-01 00:06:29+00</td>\n",
       "      <td>2025-03-03 14:28:52+00</td>\n",
       "      <td>2025-01-03 00:00:00+00</td>\n",
       "      <td>2025-03-03 14:25:15+00</td>\n",
       "      <td>5902 JEFFERSON ST</td>\n",
       "      <td>19151.0</td>\n",
       "      <td>https://d17aqltn7cihbm.cloudfront.net/uploads/...</td>\n",
       "      <td>39.978809</td>\n",
       "      <td>-75.239265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5056955</td>\n",
       "      <td>17346524</td>\n",
       "      <td>Graffiti Removal</td>\n",
       "      <td>Closed</td>\n",
       "      <td>Issue Resolved</td>\n",
       "      <td>Graffiti Removal</td>\n",
       "      <td>SR-CL01</td>\n",
       "      <td>Community Life Improvement Program</td>\n",
       "      <td>7 Business Days</td>\n",
       "      <td>2025-01-01 00:06:47+00</td>\n",
       "      <td>2025-01-07 12:48:56+00</td>\n",
       "      <td>2025-01-10 00:00:00+00</td>\n",
       "      <td>2025-01-07 12:48:46+00</td>\n",
       "      <td>1921 SPRING GARDEN ST</td>\n",
       "      <td>19130.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39.963703</td>\n",
       "      <td>-75.170116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5056956</td>\n",
       "      <td>17346525</td>\n",
       "      <td>Recycling Collection</td>\n",
       "      <td>Closed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rubbish/Recyclable Material Collection</td>\n",
       "      <td>SR-ST03</td>\n",
       "      <td>Streets Department</td>\n",
       "      <td>2 Business Days</td>\n",
       "      <td>2025-01-01 00:07:43+00</td>\n",
       "      <td>2025-01-03 11:30:26+00</td>\n",
       "      <td>2025-01-03 00:00:00+00</td>\n",
       "      <td>2025-01-03 11:25:48+00</td>\n",
       "      <td>6739 RUTLAND ST</td>\n",
       "      <td>19149.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.043401</td>\n",
       "      <td>-75.072138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   objectid  service_request_id               subject  status    status_notes  \\\n",
       "0   5056952            17346520      Graffiti Removal  Closed  Issue Resolved   \n",
       "1   5056953            17346521      Graffiti Removal  Closed  Issue Resolved   \n",
       "2   5056954            17346523  Recycling Collection  Closed             NaN   \n",
       "3   5056955            17346524      Graffiti Removal  Closed  Issue Resolved   \n",
       "4   5056956            17346525  Recycling Collection  Closed             NaN   \n",
       "\n",
       "                             service_name service_code  \\\n",
       "0                        Graffiti Removal      SR-CL01   \n",
       "1                        Graffiti Removal      SR-CL01   \n",
       "2  Rubbish/Recyclable Material Collection      SR-ST03   \n",
       "3                        Graffiti Removal      SR-CL01   \n",
       "4  Rubbish/Recyclable Material Collection      SR-ST03   \n",
       "\n",
       "                   agency_responsible   service_notice  \\\n",
       "0  Community Life Improvement Program  7 Business Days   \n",
       "1  Community Life Improvement Program  7 Business Days   \n",
       "2                  Streets Department  2 Business Days   \n",
       "3  Community Life Improvement Program  7 Business Days   \n",
       "4                  Streets Department  2 Business Days   \n",
       "\n",
       "       requested_datetime        updated_datetime       expected_datetime  \\\n",
       "0  2025-01-01 00:00:34+00  2025-01-07 12:53:27+00  2025-01-10 00:00:00+00   \n",
       "1  2025-01-01 00:03:51+00  2025-01-07 12:52:31+00  2025-01-10 00:00:00+00   \n",
       "2  2025-01-01 00:06:29+00  2025-03-03 14:28:52+00  2025-01-03 00:00:00+00   \n",
       "3  2025-01-01 00:06:47+00  2025-01-07 12:48:56+00  2025-01-10 00:00:00+00   \n",
       "4  2025-01-01 00:07:43+00  2025-01-03 11:30:26+00  2025-01-03 00:00:00+00   \n",
       "\n",
       "          closed_datetime                address  zipcode  \\\n",
       "0  2025-01-07 12:53:24+00  1701 SPRING GARDEN ST  19130.0   \n",
       "1  2025-01-07 12:52:28+00  1901 SPRING GARDEN ST  19130.0   \n",
       "2  2025-03-03 14:25:15+00      5902 JEFFERSON ST  19151.0   \n",
       "3  2025-01-07 12:48:46+00  1921 SPRING GARDEN ST  19130.0   \n",
       "4  2025-01-03 11:25:48+00        6739 RUTLAND ST  19149.0   \n",
       "\n",
       "                                           media_url        lat        lon  \n",
       "0                                                NaN  39.963169 -75.166457  \n",
       "1                                                NaN  39.963625 -75.169500  \n",
       "2  https://d17aqltn7cihbm.cloudfront.net/uploads/...  39.978809 -75.239265  \n",
       "3                                                NaN  39.963703 -75.170116  \n",
       "4                                                NaN  40.043401 -75.072138  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Philadelphia 311 data\n",
    "# Note: Using on_bad_lines='skip' to handle malformed rows in the CSV\n",
    "# Update: Data is now hosted on OneDrive for team access\n",
    "onedrive_url = \"https://falconbgsu-my.sharepoint.com/:f:/g/personal/lmoraa_bgsu_edu/IgBxvcsA2OrmQaDBpFTlbfAZAeon55WcaDPhRLTOGjI4v6c?e=fxfd82\"\n",
    "\n",
    "# If using pandas >= 1.2, you can read directly from the link if shared as a direct download\n",
    "# Otherwise, download the file manually and update the path below\n",
    "# Example for direct download (if link is direct to CSV):\n",
    "# df = pd.read_csv(onedrive_url, on_bad_lines='skip', engine='python')\n",
    "\n",
    "# For now, instruct the user to download and update the local path if needed\n",
    "print(\"NOTE: The raw data is available for download at:\")\n",
    "print(onedrive_url)\n",
    "\n",
    "# Default: Try to load from local path (update if you download to a different location)\n",
    "df = pd.read_csv(\"../data/raw/philly_311_raw.csv\", on_bad_lines='skip', engine='python')\n",
    "\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f057e9",
   "metadata": {},
   "source": [
    "## Standardize Column Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ddf75ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Standardized 18 column names\n",
      "Sample columns: ['objectid', 'service_request_id', 'subject', 'status', 'status_notes', 'service_name', 'service_code', 'agency_responsible', 'service_notice', 'requested_datetime', 'updated_datetime', 'expected_datetime', 'closed_datetime', 'address', 'zipcode', 'media_url', 'lat', 'lon']\n"
     ]
    }
   ],
   "source": [
    "# SECTION 2: Standardize Column Names (REQUIREMENT 1)\n",
    "# Rule-based column name standardization\n",
    "df.columns = (\n",
    "    df.columns\n",
    "    .str.lower()           # Convert to lowercase\n",
    "    .str.strip()           # Remove leading/trailing whitespace\n",
    "    .str.replace(\" \", \"_\")  # Replace spaces with underscores\n",
    "    .str.replace(r\"[^\\w_]\", \"\", regex=True)  # Remove special characters\n",
    ")\n",
    "\n",
    "print(f\"✓ Standardized {len(df.columns)} column names\")\n",
    "print(f\"Sample columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2f4ad8",
   "metadata": {},
   "source": [
    "## Select relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "021dc7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Selected 12 columns for Philly 311\n",
      "Dataset shape: (518841, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>service_request_id</th>\n",
       "      <th>subject</th>\n",
       "      <th>requested_datetime</th>\n",
       "      <th>service_name</th>\n",
       "      <th>service_code</th>\n",
       "      <th>service_notice</th>\n",
       "      <th>address</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>status</th>\n",
       "      <th>agency_responsible</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17346520</td>\n",
       "      <td>Graffiti Removal</td>\n",
       "      <td>2025-01-01 00:00:34+00</td>\n",
       "      <td>Graffiti Removal</td>\n",
       "      <td>SR-CL01</td>\n",
       "      <td>7 Business Days</td>\n",
       "      <td>1701 SPRING GARDEN ST</td>\n",
       "      <td>19130.0</td>\n",
       "      <td>39.963169</td>\n",
       "      <td>-75.166457</td>\n",
       "      <td>Closed</td>\n",
       "      <td>Community Life Improvement Program</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17346521</td>\n",
       "      <td>Graffiti Removal</td>\n",
       "      <td>2025-01-01 00:03:51+00</td>\n",
       "      <td>Graffiti Removal</td>\n",
       "      <td>SR-CL01</td>\n",
       "      <td>7 Business Days</td>\n",
       "      <td>1901 SPRING GARDEN ST</td>\n",
       "      <td>19130.0</td>\n",
       "      <td>39.963625</td>\n",
       "      <td>-75.169500</td>\n",
       "      <td>Closed</td>\n",
       "      <td>Community Life Improvement Program</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17346523</td>\n",
       "      <td>Recycling Collection</td>\n",
       "      <td>2025-01-01 00:06:29+00</td>\n",
       "      <td>Rubbish/Recyclable Material Collection</td>\n",
       "      <td>SR-ST03</td>\n",
       "      <td>2 Business Days</td>\n",
       "      <td>5902 JEFFERSON ST</td>\n",
       "      <td>19151.0</td>\n",
       "      <td>39.978809</td>\n",
       "      <td>-75.239265</td>\n",
       "      <td>Closed</td>\n",
       "      <td>Streets Department</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17346524</td>\n",
       "      <td>Graffiti Removal</td>\n",
       "      <td>2025-01-01 00:06:47+00</td>\n",
       "      <td>Graffiti Removal</td>\n",
       "      <td>SR-CL01</td>\n",
       "      <td>7 Business Days</td>\n",
       "      <td>1921 SPRING GARDEN ST</td>\n",
       "      <td>19130.0</td>\n",
       "      <td>39.963703</td>\n",
       "      <td>-75.170116</td>\n",
       "      <td>Closed</td>\n",
       "      <td>Community Life Improvement Program</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17346525</td>\n",
       "      <td>Recycling Collection</td>\n",
       "      <td>2025-01-01 00:07:43+00</td>\n",
       "      <td>Rubbish/Recyclable Material Collection</td>\n",
       "      <td>SR-ST03</td>\n",
       "      <td>2 Business Days</td>\n",
       "      <td>6739 RUTLAND ST</td>\n",
       "      <td>19149.0</td>\n",
       "      <td>40.043401</td>\n",
       "      <td>-75.072138</td>\n",
       "      <td>Closed</td>\n",
       "      <td>Streets Department</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   service_request_id               subject      requested_datetime  \\\n",
       "0            17346520      Graffiti Removal  2025-01-01 00:00:34+00   \n",
       "1            17346521      Graffiti Removal  2025-01-01 00:03:51+00   \n",
       "2            17346523  Recycling Collection  2025-01-01 00:06:29+00   \n",
       "3            17346524      Graffiti Removal  2025-01-01 00:06:47+00   \n",
       "4            17346525  Recycling Collection  2025-01-01 00:07:43+00   \n",
       "\n",
       "                             service_name service_code   service_notice  \\\n",
       "0                        Graffiti Removal      SR-CL01  7 Business Days   \n",
       "1                        Graffiti Removal      SR-CL01  7 Business Days   \n",
       "2  Rubbish/Recyclable Material Collection      SR-ST03  2 Business Days   \n",
       "3                        Graffiti Removal      SR-CL01  7 Business Days   \n",
       "4  Rubbish/Recyclable Material Collection      SR-ST03  2 Business Days   \n",
       "\n",
       "                 address  zipcode        lat        lon  status  \\\n",
       "0  1701 SPRING GARDEN ST  19130.0  39.963169 -75.166457  Closed   \n",
       "1  1901 SPRING GARDEN ST  19130.0  39.963625 -75.169500  Closed   \n",
       "2      5902 JEFFERSON ST  19151.0  39.978809 -75.239265  Closed   \n",
       "3  1921 SPRING GARDEN ST  19130.0  39.963703 -75.170116  Closed   \n",
       "4        6739 RUTLAND ST  19149.0  40.043401 -75.072138  Closed   \n",
       "\n",
       "                   agency_responsible  \n",
       "0  Community Life Improvement Program  \n",
       "1  Community Life Improvement Program  \n",
       "2                  Streets Department  \n",
       "3  Community Life Improvement Program  \n",
       "4                  Streets Department  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SECTION 3: Select Relevant Columns for Philly 311\n",
    "# Update this list based on the actual columns in philly_311_raw.csv\n",
    "columns_to_keep = [\n",
    "    \"service_request_id\",      # Unique identifier for Philly 311\n",
    "    \"subject\",                 # Subject of the request (if available)\n",
    "    \"requested_datetime\",      # When the request was made\n",
    "    \"service_name\",            # Type/category of service requested\n",
    "    \"service_code\",            # Service code (if available)\n",
    "    \"service_notice\",         # Service notice (if available)\n",
    "    \"address\",                 # Address of the request\n",
    "    \"zipcode\",                # ZIP code\n",
    "    \"lat\",                     # Latitude\n",
    "    \"lon\",                    # Longitude\n",
    "    \"status\",                  # Status of the request (if available)\n",
    "    \"agency_responsible\"       # Agency handling the request (if available)\n",
    "]\n",
    "\n",
    "# Only keep columns that exist in the DataFrame\n",
    "available_cols = [c for c in columns_to_keep if c in df.columns]\n",
    "missing_cols = [c for c in columns_to_keep if c not in df.columns]\n",
    "\n",
    "if missing_cols:\n",
    "    print(f\"⚠ Missing columns: {missing_cols}\")\n",
    "\n",
    "df = df[available_cols].copy()\n",
    "print(f\"✓ Selected {len(df.columns)} columns for Philly 311\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cfdceb",
   "metadata": {},
   "source": [
    "Summarized conversation history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa6d593",
   "metadata": {},
   "source": [
    "## Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f4ebe5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dropped 273033 rows with missing critical fields\n",
      "  Remaining rows: 245,808\n",
      "  Critical fields preserved: service_request_id, requested_datetime, lat\n"
     ]
    }
   ],
   "source": [
    "# SECTION 5: Handle Missing Values for Philly 311\n",
    "# 1: Drop rows with missing CRITICAL fields\n",
    "# Adjust these fields to match your Philly 311 schema\n",
    "# Only use columns that exist in the DataFrame\n",
    "possible_critical_fields = ['service_request_id', 'requested_datetime', 'lat', 'long', 'longitude', 'latitude']\n",
    "critical_fields = [col for col in possible_critical_fields if col in df.columns]\n",
    "rows_before = len(df)\n",
    "\n",
    "df = df.dropna(subset=critical_fields)\n",
    "rows_dropped_critical = rows_before - len(df)\n",
    "\n",
    "print(f\"✓ Dropped {rows_dropped_critical} rows with missing critical fields\")\n",
    "print(f\"  Remaining rows: {len(df):,}\")\n",
    "print(f\"  Critical fields preserved: {', '.join(critical_fields)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e343a61d",
   "metadata": {},
   "source": [
    "## Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "993576a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Exact duplicates removed by service_request_id: 0\n",
      "  Rows after deduplication: 245,808\n",
      "  Unique service_request_id count: 245,808\n"
     ]
    }
   ],
   "source": [
    "# Remove exact duplicates for Philly 311\n",
    "# Use the unique service_request_id if available, otherwise drop full row duplicates\n",
    "initial_rows = len(df)\n",
    "\n",
    "if 'service_request_id' in df.columns:\n",
    "    df = df.drop_duplicates(subset=['service_request_id'], keep='first')\n",
    "    exact_dupes = initial_rows - len(df)\n",
    "    print(f\"✓ Exact duplicates removed by service_request_id: {exact_dupes}\")\n",
    "else:\n",
    "    df = df.drop_duplicates(keep='first')\n",
    "    exact_dupes = initial_rows - len(df)\n",
    "    print(f\"✓ Exact full-row duplicates removed: {exact_dupes}\")\n",
    "\n",
    "print(f\"  Rows after deduplication: {len(df):,}\")\n",
    "\n",
    "if 'service_request_id' in df.columns:\n",
    "    unique_count = df['service_request_id'].nunique()\n",
    "    print(f\"  Unique service_request_id count: {unique_count:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33754f5",
   "metadata": {},
   "source": [
    "## Data Type Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb2ec683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Converted requested_datetime to datetime64\n",
      "✓ Converted 3 numeric columns to float64\n",
      "✓ Converted 3 columns to category dtype\n",
      "\n",
      "Memory usage: 77.02 MB\n"
     ]
    }
   ],
   "source": [
    "# SECTION 7: Data Type Conversion for Philly 311\n",
    "\n",
    "# Convert date columns to datetime\n",
    "if 'requested_datetime' in df.columns:\n",
    "    df['requested_datetime'] = pd.to_datetime(df['requested_datetime'], errors='coerce')\n",
    "    print(\"✓ Converted requested_datetime to datetime64\")\n",
    "\n",
    "# Convert numeric columns\n",
    "numeric_cols = ['lat', 'long', 'zip_code']\n",
    "for col in numeric_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "print(f\"✓ Converted {len(numeric_cols)} numeric columns to float64\")\n",
    "\n",
    "# Convert categorical columns (memory efficiency)\n",
    "categorical_cols = ['service_name', 'status', 'agency_responsible']\n",
    "for col in categorical_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype('category')\n",
    "print(f\"✓ Converted {len(categorical_cols)} columns to category dtype\")\n",
    "\n",
    "print(f\"\\nMemory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb056529",
   "metadata": {},
   "source": [
    "## Location Validation and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7cc517f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating coordinates with Philadelphia geographic bounds:\n",
      "  Latitude: 39.85° to 40.15°N\n",
      "  Longitude: -75.35° to -74.95°W\n",
      "\n",
      "✓ Rows with invalid coordinates: 0\n",
      "Statistical outlier detection (IQR method):\n",
      "  Latitude outliers: 0\n",
      "  Longitude outliers: 8851\n",
      "\n",
      "✓ Dataset after coordinate validation: 245,808 rows\n",
      "\n",
      "Coordinate Statistics (after validation):\n",
      "                 lat            lon\n",
      "count  245808.000000  245808.000000\n",
      "mean       39.994135     -75.150572\n",
      "std         0.048004       0.061682\n",
      "min        39.874328     -75.279832\n",
      "25%        39.954238     -75.186091\n",
      "50%        39.988049     -75.159147\n",
      "75%        40.031963     -75.120106\n",
      "max        40.137024     -74.957954\n"
     ]
    }
   ],
   "source": [
    "# SECTION 8: Location Cleaning & Geospatial Validation \n",
    "# Rule-based: Philadelphia bounding box validation\n",
    "print(\"Validating coordinates with Philadelphia geographic bounds:\")\n",
    "print(\"  Latitude: 39.85° to 40.15°N\")\n",
    "print(\"  Longitude: -75.35° to -74.95°W\")\n",
    "\n",
    "invalid_coords = (\n",
    "    (df['lat'] < 39.85) | (df['lat'] > 40.15) |\n",
    "    (df['lon'] < -75.35) | (df['lon'] > -74.95)\n",
    ")\n",
    "\n",
    "rows_invalid_coords = invalid_coords.sum()\n",
    "print(f\"\\n✓ Rows with invalid coordinates: {rows_invalid_coords}\")\n",
    "\n",
    "# Statistical: IQR-based outlier detection\n",
    "from scipy import stats\n",
    "\n",
    "def detect_outliers_iqr(data, column):\n",
    "    \"\"\"Detect outliers using Interquartile Range method\"\"\"\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return (data[column] < lower_bound) | (data[column] > upper_bound)\n",
    "\n",
    "outliers_lat = detect_outliers_iqr(df, 'lat')\n",
    "outliers_lon = detect_outliers_iqr(df, 'lon')\n",
    "\n",
    "print(f\"Statistical outlier detection (IQR method):\")\n",
    "print(f\"  Latitude outliers: {outliers_lat.sum()}\")\n",
    "print(f\"  Longitude outliers: {outliers_lon.sum()}\")\n",
    "\n",
    "# Drop invalid coordinates\n",
    "df = df[~invalid_coords].copy()\n",
    "print(f\"\\n✓ Dataset after coordinate validation: {len(df):,} rows\")\n",
    "\n",
    "# Coordinate statistics\n",
    "print(f\"\\nCoordinate Statistics (after validation):\")\n",
    "print(df[['lat', 'lon']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fe9ac28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION 9: ZIP Code Cleaning\n",
    "if 'incident_zip' in df.columns:\n",
    "    # Extract 5-digit ZIP codes\n",
    "    df['incident_zip'] = df['incident_zip'].astype(str).str.extract('(\\d{5})', expand=False)\n",
    "    \n",
    "    # Count valid vs invalid\n",
    "    valid_zips = df['incident_zip'].notna().sum()\n",
    "    invalid_zips = df['incident_zip'].isna().sum()\n",
    "    \n",
    "    print(f\"✓ ZIP code extraction (5-digit format):\")\n",
    "    print(f\"  Valid ZIP codes: {valid_zips:,}\")\n",
    "    print(f\"  Invalid/missing: {invalid_zips:,}\")\n",
    "    \n",
    "    # Convert to numeric\n",
    "    df['incident_zip'] = pd.to_numeric(df['incident_zip'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88680cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION 10: Location/District Normalization \n",
    "if 'borough' in df.columns:\n",
    "    print(\"Normalizing location/district names:\")\n",
    "    \n",
    "    # Convert to uppercase and strip whitespace\n",
    "    df['borough'] = df['borough'].str.upper().str.strip()\n",
    "    \n",
    "    # Rule-based mapping for Philadelphia districts\n",
    "    location_mapping = {\n",
    "        '1': 'CENTRAL',\n",
    "        '2': 'SOUTH',\n",
    "        '3': 'NORTHEAST',\n",
    "        '4': 'NORTH',\n",
    "        '5': 'SOUTHWEST',\n",
    "        '6': 'EAST',\n",
    "        'CENTRAL': 'CENTRAL',\n",
    "        'SOUTH': 'SOUTH',\n",
    "        'NORTHEAST': 'NORTHEAST',\n",
    "        'NORTH': 'NORTH',\n",
    "        'SOUTHWEST': 'SOUTHWEST',\n",
    "        'EAST': 'EAST'\n",
    "    }\n",
    "    \n",
    "    for old, new in location_mapping.items():\n",
    "        df['borough'] = df['borough'].replace(old, new)\n",
    "    \n",
    "    print(f\"✓ Location distribution (normalized):\")\n",
    "    print(df['borough'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d926d1d0",
   "metadata": {},
   "source": [
    "## Text Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd9df679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Service types normalized (uppercase, trimmed)\n",
      "\n",
      "Top 15 Service Types:\n",
      "service_name\n",
      "MAINTENANCE COMPLAINT                     37800\n",
      "RUBBISH/RECYCLABLE MATERIAL COLLECTION    36943\n",
      "ABANDONED VEHICLE                         27943\n",
      "ILLEGAL DUMPING                           22644\n",
      "STREET DEFECT                             16128\n",
      "INFORMATION REQUEST                       13728\n",
      "GRAFFITI REMOVAL                          10141\n",
      "STREET LIGHT OUTAGE                        8274\n",
      "SANITATION VIOLATION                       7504\n",
      "OTHER (STREETS)                            6842\n",
      "CONSTRUCTION COMPLAINTS                    6027\n",
      "LICENSE COMPLAINT                          5579\n",
      "TRAFFIC SIGNAL EMERGENCY                   5106\n",
      "STREET TREES                               4785\n",
      "DANGEROUS SIDEWALK                         3556\n",
      "Name: count, dtype: int64\n",
      "\n",
      "✓ Status values normalized\n",
      "Unique status values: 2\n",
      "status\n",
      "CLOSED    199889\n",
      "OPEN       45919\n",
      "Name: count, dtype: int64\n",
      "\n",
      "✓ Agency responsible normalized\n",
      "Unique agencies: 10\n",
      "agency_responsible\n",
      "STREETS DEPARTMENT                    124193\n",
      "LICENSE & INSPECTIONS                  44722\n",
      "POLICE DEPARTMENT                      27996\n",
      "COMMUNITY LIFE IMPROVEMENT PROGRAM     19802\n",
      "PHILLY311 CONTACT CENTER               13962\n",
      "PARKS & RECREATION                      6980\n",
      "WATER DEPARTMENT (PWD)                  3368\n",
      "FIRE DEPARTMENT                         3141\n",
      "OFFICE OF HOMELESS SERVICES             1365\n",
      "OFFICE OF INFORMATION & TECHNOLOGY       279\n",
      "Name: count, dtype: int64\n",
      "\n",
      "✓ Address normalized (uppercase, trimmed)\n"
     ]
    }
   ],
   "source": [
    "# SECTION 11: Text Normalization for Philly 311\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normalize text: uppercase, trim, remove extra spaces\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return 'UNKNOWN'\n",
    "    text = str(text).strip().upper()\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\n",
    "    return text\n",
    "\n",
    "# Normalize service_name (main complaint/service type)\n",
    "if 'service_name' in df.columns:\n",
    "    df['service_name'] = df['service_name'].apply(normalize_text)\n",
    "    print(f\"✓ Service types normalized (uppercase, trimmed)\")\n",
    "    print(f\"\\nTop 15 Service Types:\")\n",
    "    print(df['service_name'].value_counts().head(15))\n",
    "\n",
    "# Normalize status\n",
    "if 'status' in df.columns:\n",
    "    df['status'] = df['status'].apply(normalize_text)\n",
    "    print(f\"\\n✓ Status values normalized\")\n",
    "    print(f\"Unique status values: {df['status'].nunique()}\")\n",
    "    print(df['status'].value_counts())\n",
    "\n",
    "# Normalize agency_responsible\n",
    "if 'agency_responsible' in df.columns:\n",
    "    df['agency_responsible'] = df['agency_responsible'].apply(normalize_text)\n",
    "    print(f\"\\n✓ Agency responsible normalized\")\n",
    "    print(f\"Unique agencies: {df['agency_responsible'].nunique()}\")\n",
    "    print(df['agency_responsible'].value_counts().head(10))\n",
    "\n",
    "# Normalize address\n",
    "if 'address' in df.columns:\n",
    "    df['address'] = df['address'].apply(normalize_text)\n",
    "    print(f\"\\n✓ Address normalized (uppercase, trimmed)\")\n",
    "\n",
    "# Normalize zip_code\n",
    "if 'zip_code' in df.columns:\n",
    "    df['zip_code'] = df['zip_code'].astype(str).str.extract(r'(\\d{5})', expand=False)\n",
    "    print(f\"\\n✓ ZIP codes normalized to 5-digit strings\")\n",
    "    print(df['zip_code'].value_counts().head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b368677",
   "metadata": {},
   "source": [
    "## Advanced Complaint Type Normalization\n",
    "### Rule-Based Category Mapping\n",
    "Using domain knowledge and statistical analysis to group similar complaint types into standardized categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "889dfd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Service types mapped to broad categories (service_category)\n",
      "\n",
      "Service Category Distribution:\n",
      "service_category\n",
      "OTHER             176806\n",
      "ENVIRONMENTAL      32785\n",
      "PUBLIC SAFETY      27943\n",
      "INFRASTRUCTURE      8274\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total unique service types: 47\n",
      "Total unique categories: 4\n"
     ]
    }
   ],
   "source": [
    "# Advanced Complaint Type Normalization for Philly 311\n",
    "# Group similar service types into broader categories for analysis\n",
    "\n",
    "# Example mapping: (Update this mapping based on your data and analysis)\n",
    "service_type_mapping = {\n",
    "    'ILLEGAL DUMPING': 'ENVIRONMENTAL',\n",
    "    'TRASH COLLECTION': 'ENVIRONMENTAL',\n",
    "    'GRAFFITI REMOVAL': 'ENVIRONMENTAL',\n",
    "    'STREET LIGHT OUTAGE': 'INFRASTRUCTURE',\n",
    "    'POTHOLE': 'INFRASTRUCTURE',\n",
    "    'ABANDONED VEHICLE': 'PUBLIC SAFETY',\n",
    "    'NOISE COMPLAINT': 'QUALITY OF LIFE',\n",
    "    # Add more mappings as needed\n",
    "}\n",
    "\n",
    "def map_service_type(service):\n",
    "    if pd.isna(service):\n",
    "        return 'OTHER'\n",
    "    return service_type_mapping.get(service, 'OTHER')\n",
    "\n",
    "if 'service_name' in df.columns:\n",
    "    df['service_category'] = df['service_name'].apply(map_service_type)\n",
    "    print(\"✓ Service types mapped to broad categories (service_category)\")\n",
    "    print(\"\\nService Category Distribution:\")\n",
    "    print(df['service_category'].value_counts())\n",
    "    print(f\"\\nTotal unique service types: {df['service_name'].nunique()}\")\n",
    "    print(f\"Total unique categories: {df['service_category'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9993f7a",
   "metadata": {},
   "source": [
    "## Outlier Detection: Statistical Methods\n",
    "### Using IQR (Interquartile Range) and Z-Score for anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9816205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier Detection: Statistical Methods for Philly 311\n",
    "# (This section is a placeholder. Adjust columns as needed for your data.)\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# Detect spatial outliers using IQR method\n",
    "def detect_outliers_iqr(data, column):\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return (data[column] < lower_bound) | (data[column] > upper_bound)\n",
    "\n",
    "# Example: Check for latitude/longitude outliers if present\n",
    "if 'lat' in df.columns and 'long' in df.columns:\n",
    "    outliers_lat = detect_outliers_iqr(df, 'lat')\n",
    "    outliers_long = detect_outliers_iqr(df, 'long')\n",
    "    print(f\"Latitude outliers (IQR method): {outliers_lat.sum()}\")\n",
    "    print(f\"Longitude outliers (IQR method): {outliers_long.sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bea77c2",
   "metadata": {},
   "source": [
    "## Near Duplicate Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de47d1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Near-duplicate requests flagged: 5778\n",
      "    service_request_id                                            subject  \\\n",
      "5             17346526                               Recycling Collection   \n",
      "20            17346545                                   Graffiti Removal   \n",
      "36            17346576  What numbers do I call to contact PECO and Ver...   \n",
      "37            17346577           What calls are considered 911 transfers?   \n",
      "50            17346600                                     Pothole Repair   \n",
      "\n",
      "          requested_datetime                            service_name  \\\n",
      "5  2025-01-01 00:08:58+00:00  RUBBISH/RECYCLABLE MATERIAL COLLECTION   \n",
      "20 2025-01-01 00:38:43+00:00                        GRAFFITI REMOVAL   \n",
      "36 2025-01-01 04:26:58+00:00                     INFORMATION REQUEST   \n",
      "37 2025-01-01 04:32:45+00:00                     INFORMATION REQUEST   \n",
      "50 2025-01-01 12:26:13+00:00                           STREET DEFECT   \n",
      "\n",
      "   service_code    service_notice          address  zipcode        lat  \\\n",
      "5       SR-ST03   2 Business Days  6739 RUTLAND ST  19149.0  40.043401   \n",
      "20      SR-CL01   7 Business Days    912 S 46TH ST  19143.0  39.948413   \n",
      "36      SR-IR01               NaN   2054 CARVER ST  19124.0  40.015815   \n",
      "37      SR-IR01               NaN   2053 CARVER ST  19124.0  40.016011   \n",
      "50      SR-ST01  46 Business Days    222 S 51ST ST  19139.0  39.956021   \n",
      "\n",
      "          lon  status                  agency_responsible service_category  \\\n",
      "5  -75.072138  CLOSED                  STREETS DEPARTMENT            OTHER   \n",
      "20 -75.214677  CLOSED  COMMUNITY LIFE IMPROVEMENT PROGRAM    ENVIRONMENTAL   \n",
      "36 -75.065424  CLOSED            PHILLY311 CONTACT CENTER            OTHER   \n",
      "37 -75.065144  CLOSED            PHILLY311 CONTACT CENTER            OTHER   \n",
      "50 -75.223793  CLOSED                  STREETS DEPARTMENT            OTHER   \n",
      "\n",
      "    near_duplicate  \n",
      "5             True  \n",
      "20            True  \n",
      "36            True  \n",
      "37            True  \n",
      "50            True  \n"
     ]
    }
   ],
   "source": [
    "# SECTION 12: Near-Duplicate Detection (Philly 311)\n",
    "# Identify potential near-duplicate service requests based on location, time, and service type\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "# Parameters for near-duplicate detection\n",
    "TIME_WINDOW = timedelta(hours=2)  # Requests within 2 hours\n",
    "DISTANCE_THRESHOLD = 0.001        # ~100 meters if using decimal degrees (approximate for Philly)\n",
    "\n",
    "# Only run if required columns exist\n",
    "required_cols = {'lat', 'lon', 'service_name', 'requested_datetime'}\n",
    "if required_cols.issubset(df.columns):\n",
    "    # Sort by time for efficient comparison\n",
    "    df = df.sort_values('requested_datetime')\n",
    "    df['near_duplicate'] = False\n",
    "\n",
    "    # Simple pairwise check (O(n^2), can be optimized for large datasets)\n",
    "    for i in range(1, len(df)):\n",
    "        prev = df.iloc[i-1]\n",
    "        curr = df.iloc[i]\n",
    "        # Check if same service type and within time window\n",
    "        if (\n",
    "            curr['service_name'] == prev['service_name'] and\n",
    "            abs((curr['requested_datetime'] - prev['requested_datetime']).total_seconds()) <= TIME_WINDOW.total_seconds() and\n",
    "            abs(curr['lat'] - prev['lat']) <= DISTANCE_THRESHOLD and\n",
    "            abs(curr['lon'] - prev['lon']) <= DISTANCE_THRESHOLD\n",
    "        ):\n",
    "            df.iloc[i, df.columns.get_loc('near_duplicate')] = True\n",
    "\n",
    "    near_dupe_count = df['near_duplicate'].sum()\n",
    "    print(f\"✓ Near-duplicate requests flagged: {near_dupe_count}\")\n",
    "    print(df[df['near_duplicate']].head())\n",
    "else:\n",
    "    print(\"Near-duplicate detection skipped: required columns missing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f399f8",
   "metadata": {},
   "source": [
    "## Missing Value Handling: Documented Strategy\n",
    "### Four-tier approach: Drop Critical → Impute → Fill → Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6aaf8423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling categorical missing values with defaults:\n",
      "Remaining missing values:\n",
      "service_code      68072\n",
      "service_notice    15446\n",
      "zipcode            4352\n",
      "dtype: int64\n",
      "\n",
      "Dataset shape: (245808, 14)\n",
      "Total missing cells: 87870\n"
     ]
    }
   ],
   "source": [
    "# Analyze missing values BEFORE handling\n",
    "\n",
    "#2: Fill categorical fields with semantic defaults\n",
    "categorical_impute = {\n",
    "    'problem_formerly_complaint_type': 'UNKNOWN',\n",
    "    'agency': 'VARIOUS',\n",
    "    'location_type': 'UNSPECIFIED',\n",
    "    'borough': 'UNSPECIFIED'\n",
    "}\n",
    "\n",
    "print(\"Filling categorical missing values with defaults:\")\n",
    "for col, fill_value in categorical_impute.items():\n",
    "    if col in df.columns and df[col].isnull().sum() > 0:\n",
    "        fill_count = df[col].isnull().sum()\n",
    "        df[col] = df[col].fillna(fill_value)\n",
    "        print(f\"  {col}: Filled {fill_count} missing values with '{fill_value}'\")\n",
    "    elif col in df.columns:\n",
    "        print(f\"  {col}: No missing values\")\n",
    "\n",
    "# SECTION 14: Missing Value Handling - TIER 3 (TEXT)\n",
    "# 3: Fill text descriptions\n",
    "if 'problem_detail_formerly_descriptor' in df.columns:\n",
    "    missing_count = df['problem_detail_formerly_descriptor'].isnull().sum()\n",
    "    df['problem_detail_formerly_descriptor'] = df['problem_detail_formerly_descriptor'].fillna('No description provided')\n",
    "    print(f\"✓ problem_detail_formerly_descriptor: Filled {missing_count} with default text\")\n",
    "\n",
    "# SECTION 15: Missing Value Handling - TIER 4 (STATISTICAL IMPUTATION)\n",
    "# 4: Statistical imputation for incident_zip using borough median\n",
    "if 'incident_zip' in df.columns:\n",
    "    print(\"Statistical imputation strategy: Borough-level median\")\n",
    "    print(\"\\nFilling missing ZIP codes by borough:\")\n",
    "    \n",
    "    for borough in df['borough'].unique():\n",
    "        if borough != 'UNSPECIFIED':\n",
    "            # Calculate borough median ZIP\n",
    "            borough_median_zip = df[df['borough'] == borough]['incident_zip'].median()\n",
    "            \n",
    "            # Create mask for missing values in this borough\n",
    "            mask = (df['borough'] == borough) & (df['incident_zip'].isnull())\n",
    "            filled_count = mask.sum()\n",
    "            \n",
    "            # Impute if median exists\n",
    "            if not pd.isna(borough_median_zip) and filled_count > 0:\n",
    "                df.loc[mask, 'incident_zip'] = int(borough_median_zip)\n",
    "                print(f\"  {borough}: Filled {filled_count} ZIPs with median {int(borough_median_zip)}\")\n",
    "            elif filled_count > 0:\n",
    "                print(f\"  {borough}: No valid ZIPs found for imputation\")\n",
    "\n",
    "# SECTION 16: Analyze Missing Values (After Cleaning)\n",
    "missing_after = df.isnull().sum()\n",
    "missing_after = missing_after[missing_after > 0].sort_values(ascending=False)\n",
    "\n",
    "if len(missing_after) > 0:\n",
    "    print(\"Remaining missing values:\")\n",
    "    print(missing_after)\n",
    "else:\n",
    "    print(\"✓ No missing values remaining!\")\n",
    "\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Total missing cells: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e02353",
   "metadata": {},
   "source": [
    "## Data Quality Metrics and Final Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47c22d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. COMPLETENESS:\n",
      "   ✓ Complete rows (0 missing values): 162,614 (66.2%)\n",
      "   Total missing cells: 87870\n",
      "\n",
      "2. VALIDITY (Geospatial): lat/long columns not found.\n",
      "\n",
      "3. UNIQUENESS:\n",
      "   ✓ Unique service_request_id: 245,808 out of 245,808 (100.0%)\n",
      "\n",
      "4. CONSISTENCY:\n",
      "   ✓ Standardized column names: All lowercase with underscores\n",
      "   ✓ Normalized service types: 47 unique types\n",
      "   ✓ Standardized date format: datetime64[ns]\n",
      "\n",
      "5. COVERAGE:\n",
      "   Time span: 2025-01-01 to 2025-12-31\n",
      "   Agencies: 10 agencies\n",
      "   Service types: 47 types\n",
      "\n",
      "7. SERVICE CATEGORY DISTRIBUTION (Top 10):\n",
      "service_category\n",
      "OTHER             176806\n",
      "ENVIRONMENTAL      32785\n",
      "PUBLIC SAFETY      27943\n",
      "INFRASTRUCTURE      8274\n",
      "Name: count, dtype: int64\n",
      "\n",
      "8. AGENCY DISTRIBUTION (Top 10):\n",
      "agency_responsible\n",
      "STREETS DEPARTMENT                    124193\n",
      "LICENSE & INSPECTIONS                  44722\n",
      "POLICE DEPARTMENT                      27996\n",
      "COMMUNITY LIFE IMPROVEMENT PROGRAM     19802\n",
      "PHILLY311 CONTACT CENTER               13962\n",
      "PARKS & RECREATION                      6980\n",
      "WATER DEPARTMENT (PWD)                  3368\n",
      "FIRE DEPARTMENT                         3141\n",
      "OFFICE OF HOMELESS SERVICES             1365\n",
      "OFFICE OF INFORMATION & TECHNOLOGY       279\n",
      "Name: count, dtype: int64\n",
      "\n",
      "9. TEMPORAL STATISTICS:\n",
      "   Date range: 364 days\n",
      "   Records per day (avg): 673\n"
     ]
    }
   ],
   "source": [
    "# SECTION 17: Final Data Quality Assessment (Philly 311)\n",
    "\n",
    "# 1. Completeness\n",
    "complete_rows = len(df[df.isnull().sum(axis=1) == 0])\n",
    "completeness = (complete_rows / len(df)) * 100 if len(df) > 0 else 0\n",
    "print(f\"\\n1. COMPLETENESS:\")\n",
    "print(f\"   ✓ Complete rows (0 missing values): {complete_rows:,} ({completeness:.1f}%)\")\n",
    "print(f\"   Total missing cells: {df.isnull().sum().sum()}\")\n",
    "\n",
    "# 2. Validity (Geospatial)\n",
    "if set(['lat', 'long']).issubset(df.columns):\n",
    "    valid_coords = ((df['lat'] >= 39.85) & (df['lat'] <= 40.15) &\n",
    "                    (df['long'] >= -75.35) & (df['long'] <= -74.95)).sum()\n",
    "    validity_coords = (valid_coords / len(df)) * 100 if len(df) > 0 else 0\n",
    "    print(f\"\\n2. VALIDITY (Geospatial):\")\n",
    "    print(f\"   ✓ Valid Philadelphia coordinates: {valid_coords:,} ({validity_coords:.1f}%)\")\n",
    "    print(f\"   Coordinate range:\")\n",
    "    print(f\"     Latitude:  {df['lat'].min():.4f}° to {df['lat'].max():.4f}°\")\n",
    "    print(f\"     Longitude: {df['long'].min():.4f}° to {df['long'].max():.4f}°\")\n",
    "else:\n",
    "    print(\"\\n2. VALIDITY (Geospatial): lat/long columns not found.\")\n",
    "\n",
    "# 3. Uniqueness\n",
    "if 'service_request_id' in df.columns:\n",
    "    unique_ids = df['service_request_id'].nunique()\n",
    "    uniqueness = (unique_ids / len(df)) * 100 if len(df) > 0 else 0\n",
    "    print(f\"\\n3. UNIQUENESS:\")\n",
    "    print(f\"   ✓ Unique service_request_id: {unique_ids:,} out of {len(df):,} ({uniqueness:.1f}%)\")\n",
    "else:\n",
    "    print(\"\\n3. UNIQUENESS: service_request_id column not found.\")\n",
    "\n",
    "# 4. Consistency\n",
    "print(f\"\\n4. CONSISTENCY:\")\n",
    "print(f\"   ✓ Standardized column names: All lowercase with underscores\")\n",
    "if 'service_name' in df.columns:\n",
    "    print(f\"   ✓ Normalized service types: {df['service_name'].nunique()} unique types\")\n",
    "if 'requested_datetime' in df.columns:\n",
    "    print(f\"   ✓ Standardized date format: datetime64[ns]\")\n",
    "if set(['lat', 'long']).issubset(df.columns):\n",
    "    print(f\"   ✓ Normalized coordinates: float64 within Philadelphia bounds\")\n",
    "\n",
    "# 5. Coverage\n",
    "print(f\"\\n5. COVERAGE:\")\n",
    "if 'requested_datetime' in df.columns:\n",
    "    min_date = df['requested_datetime'].min()\n",
    "    max_date = df['requested_datetime'].max()\n",
    "    print(f\"   Time span: {min_date.date() if pd.notnull(min_date) else 'N/A'} to {max_date.date() if pd.notnull(max_date) else 'N/A'}\")\n",
    "if 'borough' in df.columns:\n",
    "    print(f\"   Boroughs: {df['borough'].nunique()} boroughs\")\n",
    "if 'agency_responsible' in df.columns:\n",
    "    print(f\"   Agencies: {df['agency_responsible'].nunique()} agencies\")\n",
    "if 'service_name' in df.columns:\n",
    "    print(f\"   Service types: {df['service_name'].nunique()} types\")\n",
    "\n",
    "# 6. Distribution Statistics\n",
    "if 'borough' in df.columns:\n",
    "    print(f\"\\n6. GEOGRAPHIC DISTRIBUTION (Top Boroughs):\")\n",
    "    print(df['borough'].value_counts())\n",
    "if 'service_category' in df.columns:\n",
    "    print(f\"\\n7. SERVICE CATEGORY DISTRIBUTION (Top 10):\")\n",
    "    print(df['service_category'].value_counts().head(10))\n",
    "if 'agency_responsible' in df.columns:\n",
    "    print(f\"\\n8. AGENCY DISTRIBUTION (Top 10):\")\n",
    "    print(df['agency_responsible'].value_counts().head(10))\n",
    "\n",
    "# 9. Temporal Statistics\n",
    "if 'requested_datetime' in df.columns:\n",
    "    min_date = df['requested_datetime'].min()\n",
    "    max_date = df['requested_datetime'].max()\n",
    "    if pd.notnull(min_date) and pd.notnull(max_date):\n",
    "        days_covered = (max_date - min_date).days\n",
    "        records_per_day = len(df) / (days_covered + 1) if days_covered > 0 else 0\n",
    "        print(f\"\\n9. TEMPORAL STATISTICS:\")\n",
    "        print(f\"   Date range: {days_covered} days\")\n",
    "        print(f\"   Records per day (avg): {records_per_day:.0f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b15efc",
   "metadata": {},
   "source": [
    "## Save Cleaned Data and Generate Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a953f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data source for this project (OneDrive):\n",
      "https://falconbgsu-my.sharepoint.com/:f:/g/personal/lmoraa_bgsu_edu/IgBxvcsA2OrmQaDBpFTlbfAZAeon55WcaDPhRLTOGjI4v6c?e=fxfd82\n",
      "\n",
      "✓ Cleaned data saved to: ../data/processed/philly_311_cleaned.csv\n",
      "\n",
      "========== CLEANING SUMMARY ==========\n",
      "Final row count: 245,808\n",
      "Final column count: 14\n",
      "Memory usage: 92.31 MB\n"
     ]
    }
   ],
   "source": [
    "output_path = \"../data/processed/philly_311_cleaned.csv\"\n",
    "\n",
    "# OneDrive link for team data access\n",
    "onedrive_url = \"https://falconbgsu-my.sharepoint.com/:f:/g/personal/lmoraa_bgsu_edu/IgBxvcsA2OrmQaDBpFTlbfAZAeon55WcaDPhRLTOGjI4v6c?e=fxfd82\"\n",
    "\n",
    "print(f\"Data source for this project (OneDrive):\\n{onedrive_url}\\n\")\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"✓ Cleaned data saved to: {output_path}\")\n",
    "print(f\"\\n========== CLEANING SUMMARY ==========\")\n",
    "print(f\"Final row count: {len(df):,}\")\n",
    "print(f\"Final column count: {len(df.columns)}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS4630Proj1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
