{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f9e9256",
   "metadata": {},
   "source": [
    "# NYC 311 Data Cleaning Pipeline\n",
    "This notebook implements comprehensive data cleaning for the NYC 311 Service Requests dataset including:\n",
    "- Missing value handling\n",
    "- Deduplication\n",
    "- Column standardization\n",
    "- Data type conversion\n",
    "- Location validation\n",
    "- Text normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "691d083b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227a2745",
   "metadata": {},
   "source": [
    "## Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "bb119a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (13262, 44)\n",
      "Memory usage: 29.91 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique Key</th>\n",
       "      <th>Created Date</th>\n",
       "      <th>Closed Date</th>\n",
       "      <th>Agency</th>\n",
       "      <th>Agency Name</th>\n",
       "      <th>Problem (formerly Complaint Type)</th>\n",
       "      <th>Problem Detail (formerly Descriptor)</th>\n",
       "      <th>Additional Details</th>\n",
       "      <th>Location Type</th>\n",
       "      <th>Incident Zip</th>\n",
       "      <th>...</th>\n",
       "      <th>Vehicle Type</th>\n",
       "      <th>Taxi Company Borough</th>\n",
       "      <th>Taxi Pick Up Location</th>\n",
       "      <th>Bridge Highway Name</th>\n",
       "      <th>Bridge Highway Direction</th>\n",
       "      <th>Road Ramp</th>\n",
       "      <th>Bridge Highway Segment</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>67869470</td>\n",
       "      <td>02/06/2026 02:05:09 AM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NYPD</td>\n",
       "      <td>New York City Police Department</td>\n",
       "      <td>Noise - Residential</td>\n",
       "      <td>Banging/Pounding</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Residential Building/House</td>\n",
       "      <td>11204.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.616645</td>\n",
       "      <td>-73.992191</td>\n",
       "      <td>POINT (-73.992190975602 40.616645363723)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67858781</td>\n",
       "      <td>02/06/2026 02:04:35 AM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NYPD</td>\n",
       "      <td>New York City Police Department</td>\n",
       "      <td>Noise - Residential</td>\n",
       "      <td>Banging/Pounding</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Residential Building/House</td>\n",
       "      <td>10040.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.858560</td>\n",
       "      <td>-73.929669</td>\n",
       "      <td>POINT (-73.929669194282 40.858560221857)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67860301</td>\n",
       "      <td>02/06/2026 02:04:28 AM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NYPD</td>\n",
       "      <td>New York City Police Department</td>\n",
       "      <td>Noise - Residential</td>\n",
       "      <td>Banging/Pounding</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Residential Building/House</td>\n",
       "      <td>11355.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.752019</td>\n",
       "      <td>-73.821211</td>\n",
       "      <td>POINT (-73.821211164678 40.752018968731)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>67858763</td>\n",
       "      <td>02/06/2026 02:02:03 AM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NYPD</td>\n",
       "      <td>New York City Police Department</td>\n",
       "      <td>Illegal Parking</td>\n",
       "      <td>Blocked Hydrant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Street/Sidewalk</td>\n",
       "      <td>11377.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.738983</td>\n",
       "      <td>-73.899837</td>\n",
       "      <td>POINT (-73.899837120308 40.738983283506)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>67861825</td>\n",
       "      <td>02/06/2026 02:01:27 AM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NYPD</td>\n",
       "      <td>New York City Police Department</td>\n",
       "      <td>Noise - Street/Sidewalk</td>\n",
       "      <td>Loud Talking</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Street/Sidewalk</td>\n",
       "      <td>11693.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.604511</td>\n",
       "      <td>-73.820936</td>\n",
       "      <td>POINT (-73.820936340811 40.604511386258)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unique Key            Created Date Closed Date Agency  \\\n",
       "0    67869470  02/06/2026 02:05:09 AM         NaN   NYPD   \n",
       "1    67858781  02/06/2026 02:04:35 AM         NaN   NYPD   \n",
       "2    67860301  02/06/2026 02:04:28 AM         NaN   NYPD   \n",
       "3    67858763  02/06/2026 02:02:03 AM         NaN   NYPD   \n",
       "4    67861825  02/06/2026 02:01:27 AM         NaN   NYPD   \n",
       "\n",
       "                       Agency Name Problem (formerly Complaint Type)  \\\n",
       "0  New York City Police Department               Noise - Residential   \n",
       "1  New York City Police Department               Noise - Residential   \n",
       "2  New York City Police Department               Noise - Residential   \n",
       "3  New York City Police Department                   Illegal Parking   \n",
       "4  New York City Police Department           Noise - Street/Sidewalk   \n",
       "\n",
       "  Problem Detail (formerly Descriptor) Additional Details  \\\n",
       "0                     Banging/Pounding                NaN   \n",
       "1                     Banging/Pounding                NaN   \n",
       "2                     Banging/Pounding                NaN   \n",
       "3                      Blocked Hydrant                NaN   \n",
       "4                         Loud Talking                NaN   \n",
       "\n",
       "                Location Type  Incident Zip  ... Vehicle Type  \\\n",
       "0  Residential Building/House       11204.0  ...          NaN   \n",
       "1  Residential Building/House       10040.0  ...          NaN   \n",
       "2  Residential Building/House       11355.0  ...          NaN   \n",
       "3             Street/Sidewalk       11377.0  ...          NaN   \n",
       "4             Street/Sidewalk       11693.0  ...          NaN   \n",
       "\n",
       "  Taxi Company Borough Taxi Pick Up Location Bridge Highway Name  \\\n",
       "0                  NaN                   NaN                 NaN   \n",
       "1                  NaN                   NaN                 NaN   \n",
       "2                  NaN                   NaN                 NaN   \n",
       "3                  NaN                   NaN                 NaN   \n",
       "4                  NaN                   NaN                 NaN   \n",
       "\n",
       "  Bridge Highway Direction Road Ramp Bridge Highway Segment   Latitude  \\\n",
       "0                      NaN       NaN                    NaN  40.616645   \n",
       "1                      NaN       NaN                    NaN  40.858560   \n",
       "2                      NaN       NaN                    NaN  40.752019   \n",
       "3                      NaN       NaN                    NaN  40.738983   \n",
       "4                      NaN       NaN                    NaN  40.604511   \n",
       "\n",
       "   Longitude                                  Location  \n",
       "0 -73.992191  POINT (-73.992190975602 40.616645363723)  \n",
       "1 -73.929669  POINT (-73.929669194282 40.858560221857)  \n",
       "2 -73.821211  POINT (-73.821211164678 40.752018968731)  \n",
       "3 -73.899837  POINT (-73.899837120308 40.738983283506)  \n",
       "4 -73.820936  POINT (-73.820936340811 40.604511386258)  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load NYC 311 data\n",
    "# Note: Using on_bad_lines='skip' to handle malformed rows in the CSV\n",
    "df = pd.read_csv(\"../data/raw/nyc_311/nyc_311_raw.csv\", \n",
    "                  on_bad_lines='skip',\n",
    "                  engine='python')\n",
    "\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f057e9",
   "metadata": {},
   "source": [
    "## Standardize Column Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4ddf75ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Standardized 44 column names\n",
      "Sample columns: ['unique_key', 'created_date', 'closed_date', 'agency', 'agency_name']\n"
     ]
    }
   ],
   "source": [
    "# SECTION 2: Standardize Column Names (REQUIREMENT 1)\n",
    "# Rule-based column name standardization\n",
    "df.columns = (\n",
    "    df.columns\n",
    "    .str.lower()           # Convert to lowercase\n",
    "    .str.strip()           # Remove leading/trailing whitespace\n",
    "    .str.replace(\" \", \"_\")  # Replace spaces with underscores\n",
    "    .str.replace(r\"[^\\w_]\", \"\", regex=True)  # Remove special characters\n",
    ")\n",
    "\n",
    "print(f\"✓ Standardized {len(df.columns)} column names\")\n",
    "print(f\"Sample columns: {list(df.columns[:5])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2f4ad8",
   "metadata": {},
   "source": [
    "## Select relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "021dc7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Selected 10 columns\n",
      "Dataset shape: (13262, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_key</th>\n",
       "      <th>created_date</th>\n",
       "      <th>problem_formerly_complaint_type</th>\n",
       "      <th>problem_detail_formerly_descriptor</th>\n",
       "      <th>borough</th>\n",
       "      <th>agency</th>\n",
       "      <th>location_type</th>\n",
       "      <th>incident_zip</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>67869470</td>\n",
       "      <td>02/06/2026 02:05:09 AM</td>\n",
       "      <td>Noise - Residential</td>\n",
       "      <td>Banging/Pounding</td>\n",
       "      <td>BROOKLYN</td>\n",
       "      <td>NYPD</td>\n",
       "      <td>Residential Building/House</td>\n",
       "      <td>11204.0</td>\n",
       "      <td>40.616645</td>\n",
       "      <td>-73.992191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67858781</td>\n",
       "      <td>02/06/2026 02:04:35 AM</td>\n",
       "      <td>Noise - Residential</td>\n",
       "      <td>Banging/Pounding</td>\n",
       "      <td>MANHATTAN</td>\n",
       "      <td>NYPD</td>\n",
       "      <td>Residential Building/House</td>\n",
       "      <td>10040.0</td>\n",
       "      <td>40.858560</td>\n",
       "      <td>-73.929669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67860301</td>\n",
       "      <td>02/06/2026 02:04:28 AM</td>\n",
       "      <td>Noise - Residential</td>\n",
       "      <td>Banging/Pounding</td>\n",
       "      <td>QUEENS</td>\n",
       "      <td>NYPD</td>\n",
       "      <td>Residential Building/House</td>\n",
       "      <td>11355.0</td>\n",
       "      <td>40.752019</td>\n",
       "      <td>-73.821211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>67858763</td>\n",
       "      <td>02/06/2026 02:02:03 AM</td>\n",
       "      <td>Illegal Parking</td>\n",
       "      <td>Blocked Hydrant</td>\n",
       "      <td>QUEENS</td>\n",
       "      <td>NYPD</td>\n",
       "      <td>Street/Sidewalk</td>\n",
       "      <td>11377.0</td>\n",
       "      <td>40.738983</td>\n",
       "      <td>-73.899837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>67861825</td>\n",
       "      <td>02/06/2026 02:01:27 AM</td>\n",
       "      <td>Noise - Street/Sidewalk</td>\n",
       "      <td>Loud Talking</td>\n",
       "      <td>QUEENS</td>\n",
       "      <td>NYPD</td>\n",
       "      <td>Street/Sidewalk</td>\n",
       "      <td>11693.0</td>\n",
       "      <td>40.604511</td>\n",
       "      <td>-73.820936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   unique_key            created_date problem_formerly_complaint_type  \\\n",
       "0    67869470  02/06/2026 02:05:09 AM             Noise - Residential   \n",
       "1    67858781  02/06/2026 02:04:35 AM             Noise - Residential   \n",
       "2    67860301  02/06/2026 02:04:28 AM             Noise - Residential   \n",
       "3    67858763  02/06/2026 02:02:03 AM                 Illegal Parking   \n",
       "4    67861825  02/06/2026 02:01:27 AM         Noise - Street/Sidewalk   \n",
       "\n",
       "  problem_detail_formerly_descriptor    borough agency  \\\n",
       "0                   Banging/Pounding   BROOKLYN   NYPD   \n",
       "1                   Banging/Pounding  MANHATTAN   NYPD   \n",
       "2                   Banging/Pounding     QUEENS   NYPD   \n",
       "3                    Blocked Hydrant     QUEENS   NYPD   \n",
       "4                       Loud Talking     QUEENS   NYPD   \n",
       "\n",
       "                location_type  incident_zip   latitude  longitude  \n",
       "0  Residential Building/House       11204.0  40.616645 -73.992191  \n",
       "1  Residential Building/House       10040.0  40.858560 -73.929669  \n",
       "2  Residential Building/House       11355.0  40.752019 -73.821211  \n",
       "3             Street/Sidewalk       11377.0  40.738983 -73.899837  \n",
       "4             Street/Sidewalk       11693.0  40.604511 -73.820936  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SECTION 3: Select Relevant Columns\n",
    "# Your 10 essential columns\n",
    "columns_to_keep = [\n",
    "    \"unique_key\",\n",
    "    \"created_date\",\n",
    "    \"problem_formerly_complaint_type\",\n",
    "    \"problem_detail_formerly_descriptor\",\n",
    "    \"borough\",\n",
    "    \"agency\",\n",
    "    \"location_type\",\n",
    "    \"incident_zip\",\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "]\n",
    "\n",
    "# Keep only available columns\n",
    "available_cols = [c for c in columns_to_keep if c in df.columns]\n",
    "missing_cols = [c for c in columns_to_keep if c not in df.columns]\n",
    "\n",
    "if missing_cols:\n",
    "    print(f\"⚠ Missing columns: {missing_cols}\")\n",
    "\n",
    "df = df[available_cols].copy()\n",
    "print(f\"✓ Selected {len(df.columns)} columns\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cfdceb",
   "metadata": {},
   "source": [
    "Summarized conversation history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa6d593",
   "metadata": {},
   "source": [
    "## Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8f4ebe5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dropped 144 rows with missing critical fields\n",
      "  Remaining rows: 13,118\n",
      "  Critical fields preserved: unique_key, created_date, latitude, longitude\n"
     ]
    }
   ],
   "source": [
    "# SECTION 5: Handle Missing Values \n",
    "#1: Drop rows with missing CRITICAL fields\n",
    "critical_fields = ['unique_key', 'created_date', 'latitude', 'longitude']\n",
    "rows_before = len(df)\n",
    "\n",
    "df = df.dropna(subset=critical_fields)\n",
    "rows_dropped_critical = rows_before - len(df)\n",
    "\n",
    "print(f\"✓ Dropped {rows_dropped_critical} rows with missing critical fields\")\n",
    "print(f\"  Remaining rows: {len(df):,}\")\n",
    "print(\"  Critical fields preserved: unique_key, created_date, latitude, longitude\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e343a61d",
   "metadata": {},
   "source": [
    "## Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "993576a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Exact duplicates removed by unique_key: 0\n",
      "  Rows after exact deduplication: 13,118\n",
      "  Unique complaint keys: 13,118\n"
     ]
    }
   ],
   "source": [
    "# Remove exact duplicates based on unique_key (if available)\n",
    "initial_rows = len(df)\n",
    "\n",
    "# Remove exact duplicates based on unique_key\n",
    "if 'unique_key' in df.columns:\n",
    "    df = df.drop_duplicates(subset=['unique_key'], keep='first')\n",
    "    exact_dupes = initial_rows - len(df)\n",
    "    print(f\"✓ Exact duplicates removed by unique_key: {exact_dupes}\")\n",
    "else:\n",
    "    exact_dupes = 0\n",
    "    print(\"⚠ Warning: unique_key column not found\")\n",
    "\n",
    "print(f\"  Rows after exact deduplication: {len(df):,}\")\n",
    "\n",
    "if 'unique_key' in df.columns:\n",
    "    unique_count = df['unique_key'].nunique()\n",
    "    print(f\"  Unique complaint keys: {unique_count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33754f5",
   "metadata": {},
   "source": [
    "## Data Type Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "cb2ec683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Converted created_date to datetime64\n",
      "✓ Converted 3 numeric columns to float64\n",
      "✓ Converted 4 columns to category dtype\n",
      "\n",
      "Memory usage: 1.47 MB\n"
     ]
    }
   ],
   "source": [
    "# SECTION 7: Data Type Conversion \n",
    "\n",
    "# Convert date columns to datetime\n",
    "if 'created_date' in df.columns:\n",
    "    df['created_date'] = pd.to_datetime(df['created_date'], errors='coerce')\n",
    "    print(\"✓ Converted created_date to datetime64\")\n",
    "\n",
    "# Convert numeric columns\n",
    "numeric_cols = ['latitude', 'longitude', 'incident_zip']\n",
    "for col in numeric_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "print(f\"✓ Converted {len(numeric_cols)} numeric columns to float64\")\n",
    "\n",
    "# Convert categorical columns (memory efficiency)\n",
    "categorical_cols = ['agency', 'problem_formerly_complaint_type', 'location_type', 'borough']\n",
    "for col in categorical_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype('category')\n",
    "print(f\"✓ Converted {len(categorical_cols)} columns to category dtype\")\n",
    "\n",
    "print(f\"\\nMemory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb056529",
   "metadata": {},
   "source": [
    "## Location Validation and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e7cc517f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating coordinates with NYC geographic bounds:\n",
      "  Latitude: 40.5° to 40.9°N\n",
      "  Longitude: -74.3° to -73.7°W\n",
      "\n",
      "✓ Rows with invalid coordinates: 62\n",
      "Statistical outlier detection (IQR method):\n",
      "  Latitude outliers: 0\n",
      "  Longitude outliers: 722\n",
      "\n",
      "✓ Dataset after coordinate validation: 13,056 rows\n",
      "\n",
      "Coordinate Statistics (after validation):\n",
      "           latitude     longitude\n",
      "count  13056.000000  13056.000000\n",
      "mean      40.736976    -73.921986\n",
      "std        0.088244      0.073243\n",
      "min       40.501312    -74.250187\n",
      "25%       40.668506    -73.960295\n",
      "50%       40.730369    -73.924705\n",
      "75%       40.822104    -73.882108\n",
      "max       40.899870    -73.701451\n"
     ]
    }
   ],
   "source": [
    "# SECTION 8: Location Cleaning & Geospatial Validation \n",
    "# Rule-based: NYC bounding box validation\n",
    "print(\"Validating coordinates with NYC geographic bounds:\")\n",
    "print(\"  Latitude: 40.5° to 40.9°N\")\n",
    "print(\"  Longitude: -74.3° to -73.7°W\")\n",
    "\n",
    "invalid_coords = (\n",
    "    (df['latitude'] < 40.5) | (df['latitude'] > 40.9) |\n",
    "    (df['longitude'] < -74.3) | (df['longitude'] > -73.7)\n",
    ")\n",
    "\n",
    "rows_invalid_coords = invalid_coords.sum()\n",
    "print(f\"\\n✓ Rows with invalid coordinates: {rows_invalid_coords}\")\n",
    "\n",
    "# Statistical: IQR-based outlier detection\n",
    "from scipy import stats\n",
    "\n",
    "def detect_outliers_iqr(data, column):\n",
    "    \"\"\"Detect outliers using Interquartile Range method\"\"\"\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return (data[column] < lower_bound) | (data[column] > upper_bound)\n",
    "\n",
    "outliers_lat = detect_outliers_iqr(df, 'latitude')\n",
    "outliers_lon = detect_outliers_iqr(df, 'longitude')\n",
    "\n",
    "print(f\"Statistical outlier detection (IQR method):\")\n",
    "print(f\"  Latitude outliers: {outliers_lat.sum()}\")\n",
    "print(f\"  Longitude outliers: {outliers_lon.sum()}\")\n",
    "\n",
    "# Drop invalid coordinates\n",
    "df = df[~invalid_coords].copy()\n",
    "print(f\"\\n✓ Dataset after coordinate validation: {len(df):,} rows\")\n",
    "\n",
    "# Coordinate statistics\n",
    "print(f\"\\nCoordinate Statistics (after validation):\")\n",
    "print(df[['latitude', 'longitude']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0fe9ac28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ZIP code extraction (5-digit format):\n",
      "  Valid ZIP codes: 13,036\n",
      "  Invalid/missing: 20\n"
     ]
    }
   ],
   "source": [
    "# SECTION 9: ZIP Code Cleaning\n",
    "if 'incident_zip' in df.columns:\n",
    "    # Extract 5-digit ZIP codes\n",
    "    df['incident_zip'] = df['incident_zip'].astype(str).str.extract('(\\d{5})', expand=False)\n",
    "    \n",
    "    # Count valid vs invalid\n",
    "    valid_zips = df['incident_zip'].notna().sum()\n",
    "    invalid_zips = df['incident_zip'].isna().sum()\n",
    "    \n",
    "    print(f\"✓ ZIP code extraction (5-digit format):\")\n",
    "    print(f\"  Valid ZIP codes: {valid_zips:,}\")\n",
    "    print(f\"  Invalid/missing: {invalid_zips:,}\")\n",
    "    \n",
    "    # Convert to numeric\n",
    "    df['incident_zip'] = pd.to_numeric(df['incident_zip'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "88680cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing borough names:\n",
      "✓ Borough distribution (normalized):\n",
      "borough\n",
      "BROOKLYN         4107\n",
      "QUEENS           3050\n",
      "BRONX            2950\n",
      "MANHATTAN        2465\n",
      "STATEN ISLAND     484\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# SECTION 10: Borough Normalization \n",
    "if 'borough' in df.columns:\n",
    "    print(\"Normalizing borough names:\")\n",
    "    \n",
    "    # Convert to uppercase and strip whitespace\n",
    "    df['borough'] = df['borough'].str.upper().str.strip()\n",
    "    \n",
    "    # Rule-based mapping for variations\n",
    "    borough_mapping = {\n",
    "        'NY': 'MANHATTAN',\n",
    "        'NEWYORK': 'MANHATTAN',\n",
    "        'KINGS': 'BROOKLYN',\n",
    "        'QUEENS': 'QUEENS',\n",
    "        'BRONX': 'BRONX',\n",
    "        'RICHMOND': 'STATEN ISLAND'\n",
    "    }\n",
    "    \n",
    "    for old, new in borough_mapping.items():\n",
    "        df['borough'] = df['borough'].replace(old, new)\n",
    "    \n",
    "    print(f\"✓ Borough distribution (normalized):\")\n",
    "    print(df['borough'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d926d1d0",
   "metadata": {},
   "source": [
    "## Text Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "bd9df679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Complaint types normalized (uppercase, trimmed)\n",
      "\n",
      "Top 15 Complaint Types:\n",
      "problem_formerly_complaint_type\n",
      "HEAT/HOT WATER              2422\n",
      "ILLEGAL PARKING             2046\n",
      "NOISE - RESIDENTIAL         1180\n",
      "BLOCKED DRIVEWAY            1148\n",
      "SNOW OR ICE                  795\n",
      "UNSANITARY CONDITION         453\n",
      "PLUMBING                     415\n",
      "WATER SYSTEM                 354\n",
      "PAINT/PLASTER                319\n",
      "DOOR/WINDOW                  240\n",
      "WATER LEAK                   238\n",
      "NOISE                        211\n",
      "GENERAL                      188\n",
      "TRAFFIC SIGNAL CONDITION     174\n",
      "ELECTRIC                     150\n",
      "Name: count, dtype: int64\n",
      "\n",
      "✓ Location types normalized\n",
      "Unique location types: 55\n",
      "location_type\n",
      "RESIDENTIAL BUILDING             4732\n",
      "STREET/SIDEWALK                  3594\n",
      "UNKNOWN                          1355\n",
      "RESIDENTIAL BUILDING/HOUSE       1266\n",
      "SIDEWALK                          754\n",
      "STREET                            700\n",
      "STORE/COMMERCIAL                  101\n",
      "BUSINESS                           65\n",
      "CLUB/BAR/RESTAURANT                62\n",
      "3+ FAMILY APARTMENT BUILDING       44\n",
      "ABOVE ADDRESS                      41\n",
      "RESTAURANT/BAR/DELI/BAKERY         36\n",
      "TAXI                               29\n",
      "PARK                               29\n",
      "3+ FAMILY APT. BUILDING            23\n",
      "SUBWAY                             20\n",
      "SCHOOL                             18\n",
      "BUILDING (NON-RESIDENTIAL)         17\n",
      "MIXED USE                          16\n",
      "COMMERCIAL BUILDING                14\n",
      "STREET/CURBSIDE                    14\n",
      "HOUSE OF WORSHIP                   14\n",
      "COMERCIAL                          11\n",
      "BUS STOP SHELTER                   10\n",
      "OTHER                              10\n",
      "3+ FAMILY MIXED USE BUILDING        9\n",
      "1-2 FAMILY DWELLING                 9\n",
      "OTHER (EXPLAIN BELOW)               8\n",
      "YARD                                8\n",
      "1-2 FAMILY MIXED USE BUILDING       6\n",
      "PARK/PLAYGROUND                     4\n",
      "PUBLIC/UNFENCED AREA                4\n",
      "PRIVATE SCHOOL                      3\n",
      "RESIDENTIAL                         3\n",
      "INSIDE                              2\n",
      "OFFICE BUILDING                     2\n",
      "HIGHWAY                             2\n",
      "VACANT BUILDING                     2\n",
      "KENNEL/ANIMAL SHELTER               2\n",
      "HOUSE AND STORE                     2\n",
      "TATTOO PARLOR                       1\n",
      "CURB                                1\n",
      "UNSPECIFIED                         1\n",
      "HOMELESS SHELTER                    1\n",
      "CATERING SERVICE                    1\n",
      "HALLWAY                             1\n",
      "OVERPASS                            1\n",
      "PARKING LOT OR GARAGE               1\n",
      "COMMERCIAL                          1\n",
      "RESIDENCE                           1\n",
      "1-3 FAMILY DWELLING                 1\n",
      "MOBILE FOOD VENDOR                  1\n",
      "COMMON AREA                         1\n",
      "PRIVATE HOUSE                       1\n",
      "CROSSWALK                           1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# SECTION 11: Text Normalization \n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normalize text: uppercase, trim, remove extra spaces\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return 'UNKNOWN'\n",
    "    text = str(text).strip().upper()\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\n",
    "    return text\n",
    "\n",
    "# Normalize complaint type\n",
    "if 'problem_formerly_complaint_type' in df.columns:\n",
    "    df['problem_formerly_complaint_type'] = df['problem_formerly_complaint_type'].apply(normalize_text)\n",
    "    print(f\"✓ Complaint types normalized (uppercase, trimmed)\")\n",
    "    print(f\"\\nTop 15 Complaint Types:\")\n",
    "    print(df['problem_formerly_complaint_type'].value_counts().head(15))\n",
    "\n",
    "# Normalize location type\n",
    "if 'location_type' in df.columns:\n",
    "    df['location_type'] = df['location_type'].apply(normalize_text)\n",
    "    print(f\"\\n✓ Location types normalized\")\n",
    "    print(f\"Unique location types: {df['location_type'].nunique()}\")\n",
    "    print(df['location_type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b368677",
   "metadata": {},
   "source": [
    "## Advanced Complaint Type Normalization\n",
    "### Rule-Based Category Mapping\n",
    "Using domain knowledge and statistical analysis to group similar complaint types into standardized categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "889dfd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Original Complaint Types:\n",
      "problem_formerly_complaint_type\n",
      "Illegal Parking             1624\n",
      "HEAT/HOT WATER              1497\n",
      "Blocked Driveway             956\n",
      "Noise - Residential          792\n",
      "Snow or Ice                  752\n",
      "UNSANITARY CONDITION         318\n",
      "Water System                 284\n",
      "PLUMBING                     277\n",
      "PAINT/PLASTER                218\n",
      "WATER LEAK                   212\n",
      "Noise                        185\n",
      "DOOR/WINDOW                  179\n",
      "GENERAL                      154\n",
      "Street Condition             144\n",
      "Dirty Condition              133\n",
      "Traffic Signal Condition     119\n",
      "ELECTRIC                     115\n",
      "FLOORING/STAIRS              110\n",
      "Noise - Commercial           105\n",
      "Abandoned Vehicle            101\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total unique complaint types: 118\n",
      "\n",
      "==================================================\n",
      "Normalized Complaint Type Distribution:\n",
      "==================================================\n",
      "complaint_type_normalized\n",
      "PARKING                 2580\n",
      "OTHER                   2508\n",
      "HEAT/HOT WATER          1497\n",
      "NOISE                   1256\n",
      "WEATHER                  752\n",
      "UNSANITARY CONDITION     318\n",
      "PLUMBING                 292\n",
      "WATER_UTILITY            288\n",
      "PAINT/PLASTER            218\n",
      "WATER LEAK               212\n",
      "DOOR/WINDOW              179\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total normalized categories: 11\n"
     ]
    }
   ],
   "source": [
    "# Analyze complaint type distribution before normalization\n",
    "print(\"Top 20 Original Complaint Types:\")\n",
    "complaint_counts = df_dedup['problem_formerly_complaint_type'].value_counts()\n",
    "print(complaint_counts.head(20))\n",
    "print(f\"\\nTotal unique complaint types: {df_dedup['problem_formerly_complaint_type'].nunique()}\")\n",
    "\n",
    "# Rule-based normalization mapping\n",
    "complaint_mapping = {\n",
    "    'Noise - Residential': 'NOISE',\n",
    "    'Noise - Commercial': 'NOISE',\n",
    "    'Noise - Street/Sidewalk': 'NOISE',\n",
    "    'Noise - Park': 'NOISE',\n",
    "    'Noise - Vehicle': 'NOISE',\n",
    "    'Noise': 'NOISE',\n",
    "    \n",
    "    'Illegal Parking': 'PARKING',\n",
    "    'Blocked Driveway': 'PARKING',\n",
    "    'Blocked Sidewalk': 'PARKING',\n",
    "    'Blocked Hydrant': 'PARKING',\n",
    "    'Posted Parking Sign Violation': 'PARKING',\n",
    "    \n",
    "    'Street Condition': 'STREET_CONDITION',\n",
    "    'Pothole': 'STREET_CONDITION',\n",
    "    'Street/Sidewalk Condition': 'STREET_CONDITION',\n",
    "    'Pavement Condition': 'STREET_CONDITION',\n",
    "    'Curb Condition': 'STREET_CONDITION',\n",
    "    \n",
    "    'Traffic Signal': 'TRAFFIC',\n",
    "    'Traffic Control': 'TRAFFIC',\n",
    "    'Traffic': 'TRAFFIC',\n",
    "    \n",
    "    'Street Light': 'STREET_LIGHT',\n",
    "    'Street Lights': 'STREET_LIGHT',\n",
    "    'Lighting': 'STREET_LIGHT',\n",
    "    \n",
    "    'Graffiti': 'GRAFFITI',\n",
    "    'Graffiti - Public': 'GRAFFITI',\n",
    "    'Graffiti - Private': 'GRAFFITI',\n",
    "    \n",
    "    'Sanitation': 'SANITATION',\n",
    "    'Sanitation Worker or Vehicle Complaint': 'SANITATION',\n",
    "    'Dirty Conditions': 'SANITATION',\n",
    "    'Filthy Condition': 'SANITATION',\n",
    "    'Inadequate Waste Containers': 'SANITATION',\n",
    "    \n",
    "    'Water System': 'WATER_UTILITY',\n",
    "    'Water': 'WATER_UTILITY',\n",
    "    'Water Quality': 'WATER_UTILITY',\n",
    "    \n",
    "    'Snow or Ice': 'WEATHER',\n",
    "    'Ice/Snow': 'WEATHER',\n",
    "}\n",
    "\n",
    "# Normalize complaint types\n",
    "df_dedup['complaint_type_normalized'] = df_dedup['problem_formerly_complaint_type'].str.upper().str.strip()\n",
    "\n",
    "# Apply mapping\n",
    "for original, normalized in complaint_mapping.items():\n",
    "    df_dedup.loc[df_dedup['complaint_type_normalized'] == original.upper(), 'complaint_type_normalized'] = normalized\n",
    "\n",
    "# Map remaining uncategorized to \"OTHER\"\n",
    "frequent_types = df_dedup['complaint_type_normalized'].value_counts().head(10).index\n",
    "df_dedup.loc[~df_dedup['complaint_type_normalized'].isin(frequent_types), 'complaint_type_normalized'] = 'OTHER'\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Normalized Complaint Type Distribution:\")\n",
    "print(\"=\"*50)\n",
    "normalized_counts = df_dedup['complaint_type_normalized'].value_counts()\n",
    "print(normalized_counts)\n",
    "print(f\"\\nTotal normalized categories: {df_dedup['complaint_type_normalized'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9993f7a",
   "metadata": {},
   "source": [
    "## Outlier Detection: Statistical Methods\n",
    "### Using IQR (Interquartile Range) and Z-Score for anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d9816205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for spatial outliers...\n",
      "Latitude outliers (IQR method): 0\n",
      "Longitude outliers (IQR method): 594\n",
      "Rows with valid NYC coordinates: 10100/10100\n",
      "Rows with invalid coordinates (removed): 0\n",
      "Dataset after coordinate validation: 10100 records\n",
      "\n",
      "Coordinate Statistics (after cleaning):\n",
      "           latitude     longitude\n",
      "count  10100.000000  10100.000000\n",
      "mean      40.732182    -73.923104\n",
      "std        0.087581      0.076093\n",
      "min       40.501312    -74.250187\n",
      "25%       40.666237    -73.962549\n",
      "50%       40.725121    -73.925148\n",
      "75%       40.815111    -73.881684\n",
      "max       40.899870    -73.701451\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Detect spatial outliers using IQR method\n",
    "def detect_outliers_iqr(data, column):\n",
    "    \"\"\"Detect outliers using Interquartile Range method\"\"\"\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return (data[column] < lower_bound) | (data[column] > upper_bound)\n",
    "\n",
    "# Detect spatial outliers using Z-score method\n",
    "def detect_outliers_zscore(data, column, threshold=3):\n",
    "    \"\"\"Detect outliers using Z-score method\"\"\"\n",
    "    z_scores = np.abs(stats.zscore(data[column].dropna()))\n",
    "    return np.abs(stats.zscore(data[column])) > threshold\n",
    "\n",
    "# Check for coordinate anomalies\n",
    "print(\"Checking for spatial outliers...\")\n",
    "outliers_lat = detect_outliers_iqr(df_dedup, 'latitude')\n",
    "outliers_lon = detect_outliers_iqr(df_dedup, 'longitude')\n",
    "\n",
    "print(f\"Latitude outliers (IQR method): {outliers_lat.sum()}\")\n",
    "print(f\"Longitude outliers (IQR method): {outliers_lon.sum()}\")\n",
    "\n",
    "# Spatial validation: NYC bounding box\n",
    "valid_lat = (df_dedup['latitude'] >= 40.5) & (df_dedup['latitude'] <= 40.9)\n",
    "valid_lon = (df_dedup['longitude'] >= -74.3) & (df_dedup['longitude'] <= -73.7)\n",
    "valid_coords = valid_lat & valid_lon\n",
    "\n",
    "print(f\"Rows with valid NYC coordinates: {valid_coords.sum()}/{len(df_dedup)}\")\n",
    "print(f\"Rows with invalid coordinates (removed): {(~valid_coords).sum()}\")\n",
    "\n",
    "# Remove rows with invalid coordinates\n",
    "df_dedup = df_dedup[valid_coords].reset_index(drop=True)\n",
    "\n",
    "print(f\"Dataset after coordinate validation: {len(df_dedup)} records\")\n",
    "\n",
    "# Statistical summary of coordinates\n",
    "print(\"\\nCoordinate Statistics (after cleaning):\")\n",
    "print(df_dedup[['latitude', 'longitude']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bea77c2",
   "metadata": {},
   "source": [
    "## Near Duplicate Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "de47d1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Near-duplicate detection criteria:\n",
      "  Spatial threshold (Haversine): 50 meters\n",
      "  Temporal threshold: 24 hours\n",
      "  Semantic requirement: Same complaint type\n",
      "\n",
      "✓ Near-duplicate detection complete:\n",
      "  Rows checked: 13,055\n",
      "  Near-duplicates identified: 2,960\n",
      "  Rows removed: 2,960\n",
      "  Rows retained: 10,096\n"
     ]
    }
   ],
   "source": [
    "# SECTION 12: Near-Duplicate Detection \n",
    "\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calculate distance in meters between two coordinates using Haversine formula\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    lat1, lon1 : float - First coordinate (degrees)\n",
    "    lat2, lon2 : float - Second coordinate (degrees)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float : Distance in meters\n",
    "    \"\"\"\n",
    "    R = 6371000  # Earth's radius in meters\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1-a))\n",
    "    return R * c\n",
    "\n",
    "# Sort by spatial and temporal attributes for efficiency\n",
    "df = df.sort_values(['latitude', 'longitude', 'problem_formerly_complaint_type', 'created_date']).reset_index(drop=True)\n",
    "\n",
    "# Parameters for near-duplicate detection\n",
    "DISTANCE_THRESHOLD_M = 50  # meters\n",
    "TIME_THRESHOLD_H = 24      # hours\n",
    "\n",
    "print(\"Near-duplicate detection criteria:\")\n",
    "print(f\"  Spatial threshold (Haversine): {DISTANCE_THRESHOLD_M} meters\")\n",
    "print(f\"  Temporal threshold: {TIME_THRESHOLD_H} hours\")\n",
    "print(f\"  Semantic requirement: Same complaint type\")\n",
    "\n",
    "# Detect near-duplicates\n",
    "near_dup_indices = []\n",
    "rows_checked = 0\n",
    "\n",
    "for i in range(len(df) - 1):\n",
    "    current = df.iloc[i]\n",
    "    next_row = df.iloc[i + 1]\n",
    "    \n",
    "    if pd.isna(current['latitude']) or pd.isna(current['longitude']):\n",
    "        continue\n",
    "    \n",
    "    # Calculate distance\n",
    "    distance = haversine_distance(\n",
    "        current['latitude'], current['longitude'],\n",
    "        next_row['latitude'], next_row['longitude']\n",
    "    )\n",
    "    \n",
    "    # Calculate time difference\n",
    "    time_diff = abs((next_row['created_date'] - current['created_date']).total_seconds() / 3600)\n",
    "    \n",
    "    # Mark as near-duplicate if ALL criteria met\n",
    "    if (distance < DISTANCE_THRESHOLD_M and \n",
    "        time_diff < TIME_THRESHOLD_H and \n",
    "        current['problem_formerly_complaint_type'] == next_row['problem_formerly_complaint_type']):\n",
    "        near_dup_indices.append(i + 1)  # Keep first, mark second for removal\n",
    "    \n",
    "    rows_checked += 1\n",
    "\n",
    "# Remove near-duplicates\n",
    "df_before_near_dup = len(df)\n",
    "df = df[~df.index.isin(near_dup_indices)].reset_index(drop=True)\n",
    "near_dupes = df_before_near_dup - len(df)\n",
    "\n",
    "print(f\"\\n✓ Near-duplicate detection complete:\")\n",
    "print(f\"  Rows checked: {rows_checked:,}\")\n",
    "print(f\"  Near-duplicates identified: {near_dupes:,}\")\n",
    "print(f\"  Rows removed: {near_dupes:,}\")\n",
    "print(f\"  Rows retained: {len(df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f399f8",
   "metadata": {},
   "source": [
    "## Missing Value Handling: Documented Strategy\n",
    "### Four-tier approach: Drop Critical → Impute → Fill → Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6aaf8423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling categorical missing values with defaults:\n",
      "  problem_formerly_complaint_type: No missing values\n",
      "  agency: No missing values\n",
      "  location_type: No missing values\n",
      "  borough: No missing values\n",
      "✓ problem_detail_formerly_descriptor: Filled 0 with default text\n",
      "Statistical imputation strategy: Borough-level median\n",
      "\n",
      "Filling missing ZIP codes by borough:\n",
      "✓ No missing values remaining!\n",
      "\n",
      "Dataset shape: (10096, 10)\n",
      "Total missing cells: 0\n"
     ]
    }
   ],
   "source": [
    "# Analyze missing values BEFORE handling\n",
    "\n",
    "#2: Fill categorical fields with semantic defaults\n",
    "categorical_impute = {\n",
    "    'problem_formerly_complaint_type': 'UNKNOWN',\n",
    "    'agency': 'VARIOUS',\n",
    "    'location_type': 'UNSPECIFIED',\n",
    "    'borough': 'UNSPECIFIED'\n",
    "}\n",
    "\n",
    "print(\"Filling categorical missing values with defaults:\")\n",
    "for col, fill_value in categorical_impute.items():\n",
    "    if col in df.columns and df[col].isnull().sum() > 0:\n",
    "        fill_count = df[col].isnull().sum()\n",
    "        df[col] = df[col].fillna(fill_value)\n",
    "        print(f\"  {col}: Filled {fill_count} missing values with '{fill_value}'\")\n",
    "    elif col in df.columns:\n",
    "        print(f\"  {col}: No missing values\")\n",
    "\n",
    "# SECTION 14: Missing Value Handling - TIER 3 (TEXT)\n",
    "# 3: Fill text descriptions\n",
    "if 'problem_detail_formerly_descriptor' in df.columns:\n",
    "    missing_count = df['problem_detail_formerly_descriptor'].isnull().sum()\n",
    "    df['problem_detail_formerly_descriptor'] = df['problem_detail_formerly_descriptor'].fillna('No description provided')\n",
    "    print(f\"✓ problem_detail_formerly_descriptor: Filled {missing_count} with default text\")\n",
    "\n",
    "# SECTION 15: Missing Value Handling - TIER 4 (STATISTICAL IMPUTATION)\n",
    "# 4: Statistical imputation for incident_zip using borough median\n",
    "if 'incident_zip' in df.columns:\n",
    "    print(\"Statistical imputation strategy: Borough-level median\")\n",
    "    print(\"\\nFilling missing ZIP codes by borough:\")\n",
    "    \n",
    "    for borough in df['borough'].unique():\n",
    "        if borough != 'UNSPECIFIED':\n",
    "            # Calculate borough median ZIP\n",
    "            borough_median_zip = df[df['borough'] == borough]['incident_zip'].median()\n",
    "            \n",
    "            # Create mask for missing values in this borough\n",
    "            mask = (df['borough'] == borough) & (df['incident_zip'].isnull())\n",
    "            filled_count = mask.sum()\n",
    "            \n",
    "            # Impute if median exists\n",
    "            if not pd.isna(borough_median_zip) and filled_count > 0:\n",
    "                df.loc[mask, 'incident_zip'] = int(borough_median_zip)\n",
    "                print(f\"  {borough}: Filled {filled_count} ZIPs with median {int(borough_median_zip)}\")\n",
    "            elif filled_count > 0:\n",
    "                print(f\"  {borough}: No valid ZIPs found for imputation\")\n",
    "\n",
    "# SECTION 16: Analyze Missing Values (After Cleaning)\n",
    "missing_after = df.isnull().sum()\n",
    "missing_after = missing_after[missing_after > 0].sort_values(ascending=False)\n",
    "\n",
    "if len(missing_after) > 0:\n",
    "    print(\"Remaining missing values:\")\n",
    "    print(missing_after)\n",
    "else:\n",
    "    print(\"✓ No missing values remaining!\")\n",
    "\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Total missing cells: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e02353",
   "metadata": {},
   "source": [
    "## Data Quality Metrics and Final Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "47c22d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. COMPLETENESS:\n",
      "   ✓ Complete rows (0 missing values): 10,096 (100.0%)\n",
      "   Total missing cells: 0\n",
      "\n",
      "2. VALIDITY (Geospatial):\n",
      "   ✓ Valid NYC coordinates: 10,096 (100.0%)\n",
      "   Coordinate range:\n",
      "     Latitude:  40.5013° to 40.8999°\n",
      "     Longitude: -74.2502° to -73.7015°\n",
      "\n",
      "3. UNIQUENESS:\n",
      "   ✓ Unique identifiers: 10,096 out of 10,096 (100.0%)\n",
      "\n",
      "4. CONSISTENCY:\n",
      "   ✓ Standardized column names: All lowercase with underscores\n",
      "   ✓ Normalized complaint types: 116 unique types\n",
      "   ✓ Standardized date format: datetime64[ns]\n",
      "   ✓ Normalized coordinates: float64 within NYC bounds\n",
      "\n",
      "5. COVERAGE:\n",
      "   Time span: 2026-02-04 to 2026-02-06\n",
      "   Boroughs: 5 boroughs\n",
      "   Agencies: 13 agencies\n",
      "   Complaint types: 116 types\n",
      "\n",
      "6. GEOGRAPHIC DISTRIBUTION (Top Boroughs):\n",
      "borough\n",
      "BROOKLYN         3250\n",
      "QUEENS           2408\n",
      "BRONX            2117\n",
      "MANHATTAN        1892\n",
      "STATEN ISLAND     429\n",
      "Name: count, dtype: int64\n",
      "\n",
      "7. COMPLAINT TYPE DISTRIBUTION (Top 10):\n",
      "problem_formerly_complaint_type\n",
      "ILLEGAL PARKING         1624\n",
      "HEAT/HOT WATER          1497\n",
      "BLOCKED DRIVEWAY         956\n",
      "NOISE - RESIDENTIAL      792\n",
      "SNOW OR ICE              752\n",
      "UNSANITARY CONDITION     318\n",
      "PLUMBING                 291\n",
      "WATER SYSTEM             284\n",
      "PAINT/PLASTER            218\n",
      "WATER LEAK               212\n",
      "Name: count, dtype: int64\n",
      "\n",
      "8. AGENCY DISTRIBUTION (Top 10):\n",
      "agency\n",
      "NYPD     3919\n",
      "HPD      3218\n",
      "DSNY     1133\n",
      "DEP       644\n",
      "DOT       448\n",
      "DOB       221\n",
      "DOHMH     214\n",
      "DPR        99\n",
      "TLC        86\n",
      "DCWP       64\n",
      "Name: count, dtype: int64\n",
      "\n",
      "9. TEMPORAL STATISTICS:\n",
      "   Date range: 1 days\n",
      "   Records per day (avg): 5048\n"
     ]
    }
   ],
   "source": [
    "# SECTION 17: Final Data Quality Assessment\n",
    "\n",
    "# 1. Completeness\n",
    "complete_rows = len(df[df.isnull().sum(axis=1) == 0])\n",
    "completeness = (complete_rows / len(df)) * 100\n",
    "print(f\"\\n1. COMPLETENESS:\")\n",
    "print(f\"   ✓ Complete rows (0 missing values): {complete_rows:,} ({completeness:.1f}%)\")\n",
    "print(f\"   Total missing cells: {df.isnull().sum().sum()}\")\n",
    "\n",
    "# 2. Validity\n",
    "valid_coords = ((df['latitude'] >= 40.5) & (df['latitude'] <= 40.9) &\n",
    "                (df['longitude'] >= -74.3) & (df['longitude'] <= -73.7)).sum()\n",
    "validity_coords = (valid_coords / len(df)) * 100\n",
    "print(f\"\\n2. VALIDITY (Geospatial):\")\n",
    "print(f\"   ✓ Valid NYC coordinates: {valid_coords:,} ({validity_coords:.1f}%)\")\n",
    "print(f\"   Coordinate range:\")\n",
    "print(f\"     Latitude:  {df['latitude'].min():.4f}° to {df['latitude'].max():.4f}°\")\n",
    "print(f\"     Longitude: {df['longitude'].min():.4f}° to {df['longitude'].max():.4f}°\")\n",
    "\n",
    "# 3. Uniqueness\n",
    "if 'unique_key' in df.columns:\n",
    "    unique_keys = df['unique_key'].nunique()\n",
    "    uniqueness = (unique_keys / len(df)) * 100\n",
    "    print(f\"\\n3. UNIQUENESS:\")\n",
    "    print(f\"   ✓ Unique identifiers: {unique_keys:,} out of {len(df):,} ({uniqueness:.1f}%)\")\n",
    "\n",
    "# 4. Consistency\n",
    "print(f\"\\n4. CONSISTENCY:\")\n",
    "print(f\"   ✓ Standardized column names: All lowercase with underscores\")\n",
    "print(f\"   ✓ Normalized complaint types: {df['problem_formerly_complaint_type'].nunique()} unique types\")\n",
    "print(f\"   ✓ Standardized date format: datetime64[ns]\")\n",
    "print(f\"   ✓ Normalized coordinates: float64 within NYC bounds\")\n",
    "\n",
    "# 5. Coverage\n",
    "print(f\"\\n5. COVERAGE:\")\n",
    "print(f\"   Time span: {df['created_date'].min().date()} to {df['created_date'].max().date()}\")\n",
    "print(f\"   Boroughs: {df['borough'].nunique()} boroughs\")\n",
    "print(f\"   Agencies: {df['agency'].nunique()} agencies\")\n",
    "print(f\"   Complaint types: {df['problem_formerly_complaint_type'].nunique()} types\")\n",
    "\n",
    "# 6. Distribution Statistics\n",
    "print(f\"\\n6. GEOGRAPHIC DISTRIBUTION (Top Boroughs):\")\n",
    "print(df['borough'].value_counts())\n",
    "\n",
    "print(f\"\\n7. COMPLAINT TYPE DISTRIBUTION (Top 10):\")\n",
    "print(df['problem_formerly_complaint_type'].value_counts().head(10))\n",
    "\n",
    "print(f\"\\n8. AGENCY DISTRIBUTION (Top 10):\")\n",
    "print(df['agency'].value_counts().head(10))\n",
    "\n",
    "print(f\"\\n9. TEMPORAL STATISTICS:\")\n",
    "days_covered = (df['created_date'].max() - df['created_date'].min()).days\n",
    "records_per_day = len(df) / (days_covered + 1) if days_covered > 0 else 0\n",
    "print(f\"   Date range: {days_covered} days\")\n",
    "print(f\"   Records per day (avg): {records_per_day:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b15efc",
   "metadata": {},
   "source": [
    "## Save Cleaned Data and Generate Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0a953f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Cleaned data saved to: ../data/raw/nyc_311_cleaned.csv\n",
      "\n",
      "========== CLEANING SUMMARY ==========\n",
      "Final row count: 10,096\n",
      "Final column count: 10\n",
      "Memory usage: 2.80 MB\n"
     ]
    }
   ],
   "source": [
    "output_path = \"../data/raw/nyc_311_cleaned.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"✓ Cleaned data saved to: {output_path}\")\n",
    "print(f\"\\n========== CLEANING SUMMARY ==========\")\n",
    "print(f\"Final row count: {len(df):,}\")\n",
    "print(f\"Final column count: {len(df.columns)}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
