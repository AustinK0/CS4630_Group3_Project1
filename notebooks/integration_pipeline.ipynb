{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a698ffd",
   "metadata": {},
   "source": [
    "**Integration:** Urban Data Cleaning, Integration, and Enrichment with Python\n",
    "\n",
    "## Integration Summary\n",
    "This phase integrates Philly 311 complaints 2025 with Yelp business data 2005–2022 using a hybrid match. Per the last project brief, temporal alignment is intentionally ignored. The integration relies only on geospatial proximity and feature similarity.\n",
    "\n",
    "Additional information is stored [here](../INTEGRATION.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97726cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d68dbd0",
   "metadata": {},
   "source": [
    "### Define Config class for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b286cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class IntegrationConfig:\n",
    "    # Paths Philly 311 + Yelp clustered categories\n",
    "    complaint_path: str = \"../data/processed/philly_311_cleaned.csv\"\n",
    "    business_path: str = \"../data/processed/yelp_clustered_categories.json\"\n",
    "    output_path: str = \"../data/processed/final_integrated_dataset.csv\"\n",
    "\n",
    "    # Complaint columns (Philly 311)\n",
    "    complaint_id_col: str = \"service_request_id\" \n",
    "    complaint_lat_col: str = \"lat\"\n",
    "    complaint_lon_col: str = \"lon\"\n",
    "    complaint_category_col: str = \"service_category\"\n",
    "    complaint_text_col: str = \"service_name\"\n",
    "\n",
    "    # Business columns (Yelp)\n",
    "    business_id_col: str = \"business_id\"\n",
    "    business_lat_col: str = \"latitude\"\n",
    "    business_lon_col: str = \"longitude\"\n",
    "    business_category_col: str = \"normal_category\"  # K-means normalized\n",
    "    business_name_col: str = \"name\"\n",
    "\n",
    "    # Integration parameters\n",
    "    radius_m: float = 500.0  # Match businesses within 500m\n",
    "    top_k: int = 3  # Keep top 3 matches per complaint\n",
    "\n",
    "    # Hybrid scoring weights must sum to 1.0\n",
    "    weight_geo: float = 0.6\n",
    "    weight_text: float = 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f12733",
   "metadata": {},
   "source": [
    "### Define various helper methods\n",
    "These methods will be used for parts of the integration\n",
    "\n",
    "- load_dataset(path) — loads a given dataset and returns its DataFrame representation; supports JSON (lines) and CSV.\n",
    "\n",
    "- validate_columns(df, required, df_name) — throws an error if required columns are missing from the provided DataFrame.\n",
    "\n",
    "- validate_config(cfg) — throws an error if required config values (like radius, top_k, or hybrid weights) do not follow expected rules.\n",
    "\n",
    "- normalize_text(text) — cleans input text by converting to lowercase and replacing non-alphanumeric characters with spaces; returns a list of tokens.\n",
    "\n",
    "- *token_set(parts) — aggregates multiple text segments into a single unique set of normalized tokens.\n",
    "\n",
    "- jaccard_similarity(a, b) — calculates the intersection over union for two sets to determine their textual similarity.\n",
    "\n",
    "- haversine_m(lat1, lon1, lat2, lon2) — computes the great-circle distance between two points on Earth in meters using the Haversine formula.\n",
    "\n",
    "- build_grid_index(lat_values, lon_values, radius_m) — creates a spatial hash map (grid) of coordinates to optimize geospatial filtering by grouping indices into cells.\n",
    "\n",
    "- get_neighbor_cells(cell) — returns the coordinates of a target cell and its eight immediate neighbors in the spatial grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fdfbaad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path: str) -> pd.DataFrame:\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "    if path.endswith('.json'):\n",
    "        return pd.read_json(path, lines=True)\n",
    "    else:\n",
    "        return pd.read_csv(path)\n",
    "\n",
    "def validate_columns(df: pd.DataFrame, required: Iterable[str], df_name: str) -> None:\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"{df_name} missing required columns: {missing}\")\n",
    "\n",
    "def validate_config(cfg: IntegrationConfig) -> None:\n",
    "    # Validate parameters (temporal data intentionally ignored per project spec)\n",
    "    if cfg.radius_m <= 0:\n",
    "        raise ValueError(\"radius_m must be > 0\")\n",
    "    if cfg.top_k <= 0:\n",
    "        raise ValueError(\"top_k must be > 0\")\n",
    "    if cfg.weight_geo <= 0 or cfg.weight_text <= 0:\n",
    "        raise ValueError(\"Hybrid approach requires both weight_geo and weight_text > 0\")\n",
    "    weight_sum = cfg.weight_geo + cfg.weight_text\n",
    "    if not math.isclose(weight_sum, 1.0, rel_tol=1e-6):\n",
    "        raise ValueError(f\"weight_geo + weight_text must equal 1.0 (got {weight_sum})\")\n",
    "\n",
    "def normalize_text(text: Optional[str]) -> List[str]:\n",
    "    if text is None or (isinstance(text, float) and math.isnan(text)):\n",
    "        return []\n",
    "    cleaned = []\n",
    "    for ch in str(text).lower():\n",
    "        cleaned.append(ch if ch.isalnum() else \" \")\n",
    "    return [t for t in \"\".join(cleaned).split() if t]\n",
    "\n",
    "def token_set(*parts: Optional[str]) -> set:\n",
    "    tokens: List[str] = []\n",
    "    for part in parts:\n",
    "        tokens.extend(normalize_text(part))\n",
    "    return set(tokens)\n",
    "\n",
    "def jaccard_similarity(a: set, b: set) -> float:\n",
    "    if not a and not b:\n",
    "        return 0.0\n",
    "    return len(a & b) / max(1, len(a | b))\n",
    "\n",
    "def haversine_m(lat1: float, lon1: float, lat2: float, lon2: float) -> float:\n",
    "    r = 6_371_000  # Earth radius in meters\n",
    "    phi1 = math.radians(lat1)\n",
    "    phi2 = math.radians(lat2)\n",
    "    dphi = math.radians(lat2 - lat1)\n",
    "    dlambda = math.radians(lon2 - lon1)\n",
    "\n",
    "    a = math.sin(dphi / 2) ** 2 + math.cos(phi1) * math.cos(phi2) * math.sin(dlambda / 2) ** 2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    return r * c\n",
    "\n",
    "# Spatial Grid Indexing for Efficient Geospatial Filtering\n",
    "\n",
    "def build_grid_index(\n",
    "    lat_values: np.ndarray,\n",
    "    lon_values: np.ndarray,\n",
    "    radius_m: float,\n",
    ") -> Tuple[Dict[Tuple[int, int], List[int]], float, float]:\n",
    "    lat_deg = radius_m / 111_320\n",
    "    mean_lat = np.nanmean(lat_values)\n",
    "    lon_deg = radius_m / (111_320 * math.cos(math.radians(mean_lat)))\n",
    "\n",
    "    grid: Dict[Tuple[int, int], List[int]] = {}\n",
    "    for idx in range(len(lat_values)):\n",
    "        lat = lat_values[idx]\n",
    "        lon = lon_values[idx]\n",
    "        if pd.isna(lat) or pd.isna(lon):\n",
    "            continue\n",
    "        key = (int(lat / lat_deg), int(lon / lon_deg))\n",
    "        grid.setdefault(key, []).append(idx)\n",
    "\n",
    "    return grid, lat_deg, lon_deg\n",
    "\n",
    "def get_neighbor_cells(cell: Tuple[int, int]) -> List[Tuple[int, int]]:\n",
    "    (cx, cy) = cell\n",
    "    return [(cx + dx, cy + dy) for dx in (-1, 0, 1) for dy in (-1, 0, 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c489a10",
   "metadata": {},
   "source": [
    "### Integration Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6dde585b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_datasets(cfg: IntegrationConfig) -> pd.DataFrame:\n",
    "    validate_config(cfg)\n",
    "    \n",
    "    # Stage 1: Load datasets\n",
    "    print(\"\\nStage 1: Load datasets\")\n",
    "    print(f\"Complaints: {cfg.complaint_path}\")\n",
    "    complaints = load_dataset(cfg.complaint_path)\n",
    "    print(f\"Loaded complaints: {len(complaints):,}\")\n",
    "    \n",
    "    print(f\"Businesses: {cfg.business_path}\")\n",
    "    businesses = load_dataset(cfg.business_path)\n",
    "    print(f\"Loaded businesses: {len(businesses):,}\")\n",
    "\n",
    "    # Validate columns\n",
    "    validate_columns(\n",
    "        complaints,\n",
    "        [\n",
    "            cfg.complaint_id_col,\n",
    "            cfg.complaint_lat_col,\n",
    "            cfg.complaint_lon_col,\n",
    "            cfg.complaint_category_col,\n",
    "        ],\n",
    "        \"Complaint\",\n",
    "    )\n",
    "    validate_columns(\n",
    "        businesses,\n",
    "        [\n",
    "            cfg.business_id_col,\n",
    "            cfg.business_lat_col,\n",
    "            cfg.business_lon_col,\n",
    "            cfg.business_category_col,\n",
    "        ],\n",
    "        \"Business\",\n",
    "    )\n",
    "\n",
    "    # Stage 2: Build spatial index\n",
    "    print(\"\\nStage 2: Build spatial index\")\n",
    "    print(f\"Radius: {cfg.radius_m}m\")\n",
    "    \n",
    "    business_lats = businesses[cfg.business_lat_col].values\n",
    "    business_lons = businesses[cfg.business_lon_col].values\n",
    "    \n",
    "    grid, lat_deg, lon_deg = build_grid_index(business_lats, business_lons, cfg.radius_m)\n",
    "    print(f\"Grid cells: {len(grid):,}\")\n",
    "\n",
    "    # Stage 3: Tokenize\n",
    "    print(\"\\nStage 3: Tokenize\")\n",
    "    print(f\"Complaints: {len(complaints):,}\")\n",
    "    complaint_tokens = {}\n",
    "    for idx, row in complaints.iterrows():\n",
    "        complaint_tokens[idx] = token_set(\n",
    "            row.get(cfg.complaint_category_col), \n",
    "            row.get(cfg.complaint_text_col)\n",
    "        )\n",
    "    print(\"Complaint tokens: done\")\n",
    "    \n",
    "    print(f\"Businesses: {len(businesses):,}\")\n",
    "    business_tokens = {}\n",
    "    for idx in range(len(businesses)):\n",
    "        business_tokens[idx] = token_set(\n",
    "            businesses.iloc[idx].get(cfg.business_category_col), \n",
    "            businesses.iloc[idx].get(cfg.business_name_col)\n",
    "        )\n",
    "    print(\"Business tokens: done\")\n",
    "\n",
    "    # Stage 4: Match\n",
    "    print(\"\\nStage 4: Match\")\n",
    "    print(f\"Complaints to process: {len(complaints):,}\")\n",
    "    rows = []\n",
    "    matched_complaints = 0\n",
    "    \n",
    "    # Cache business data for faster access\n",
    "    business_ids = businesses[cfg.business_id_col].values\n",
    "    business_names = businesses[cfg.business_name_col].values\n",
    "    business_cats = businesses[cfg.business_category_col].values\n",
    "    \n",
    "    for c_idx, c_row in complaints.iterrows():\n",
    "        if (c_idx + 1) % 50000 == 0:\n",
    "            print(f\"Progress: {c_idx + 1:,} / {len(complaints):,}\")\n",
    "        \n",
    "        c_lat = c_row[cfg.complaint_lat_col]\n",
    "        c_lon = c_row[cfg.complaint_lon_col]\n",
    "        if pd.isna(c_lat) or pd.isna(c_lon):\n",
    "            continue\n",
    "\n",
    "        # Find neighboring grid cells for the geospatial filter\n",
    "        c_cell = (int(c_lat / lat_deg), int(c_lon / lon_deg))\n",
    "        candidate_indices: List[int] = []\n",
    "        for cell in get_neighbor_cells(c_cell):\n",
    "            candidate_indices.extend(grid.get(cell, []))\n",
    "\n",
    "        # Score candidates hybridly geospatial and semantic\n",
    "        scored: List[Tuple[int, float, float, float]] = []\n",
    "        for b_idx in candidate_indices:\n",
    "            b_lat = business_lats[b_idx]\n",
    "            b_lon = business_lons[b_idx]\n",
    "            if pd.isna(b_lat) or pd.isna(b_lon):\n",
    "                continue\n",
    "\n",
    "            # Geospatial component\n",
    "            dist = haversine_m(c_lat, c_lon, b_lat, b_lon)\n",
    "            if dist > cfg.radius_m:\n",
    "                continue\n",
    "\n",
    "            # Semantic component\n",
    "            sim = jaccard_similarity(complaint_tokens[c_idx], business_tokens[b_idx])\n",
    "            \n",
    "            # Hybrid score combine both components\n",
    "            geo_score = 1.0 - (dist / cfg.radius_m)\n",
    "            score = (cfg.weight_geo * geo_score) + (cfg.weight_text * sim)\n",
    "            scored.append((b_idx, score, dist, sim))\n",
    "\n",
    "        # Keep top k matches per complaint\n",
    "        if scored:\n",
    "            matched_complaints += 1\n",
    "            scored.sort(key=lambda x: x[1], reverse=True)\n",
    "            for b_idx, score, dist, sim in scored[: cfg.top_k]:\n",
    "                rows.append(\n",
    "                    {\n",
    "                        \"complaint_id\": c_row[cfg.complaint_id_col],\n",
    "                        \"complaint_category\": c_row.get(cfg.complaint_category_col),\n",
    "                        \"complaint_service\": c_row.get(cfg.complaint_text_col),\n",
    "                        \"complaint_lat\": c_lat,\n",
    "                        \"complaint_lon\": c_lon,\n",
    "                        \"business_id\": business_ids[b_idx],\n",
    "                        \"business_name\": business_names[b_idx],\n",
    "                        \"business_category\": business_cats[b_idx],\n",
    "                        \"business_lat\": business_lats[b_idx],\n",
    "                        \"business_lon\": business_lons[b_idx],\n",
    "                        \"distance_m\": round(dist, 2),\n",
    "                        \"similarity\": round(sim, 4),\n",
    "                        \"score\": round(score, 4),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    print(f\"Matched complaints: {matched_complaints:,}\")\n",
    "    integrated = pd.DataFrame(rows)\n",
    "    print(f\"Match records: {len(integrated):,}\")\n",
    "    return integrated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc27022",
   "metadata": {},
   "source": [
    "# Summarization Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71d093bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def summarize_matches(integrated: pd.DataFrame) -> None:\n",
    "    if integrated.empty:\n",
    "        print(\"No matches to summarize.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nMatch summary\")\n",
    "    print(f\"Total matches: {len(integrated):,}\")\n",
    "\n",
    "    if \"distance_m\" in integrated.columns:\n",
    "        print(\"\\nDistance (m):\")\n",
    "        print(integrated[\"distance_m\"].describe(percentiles=[0.25, 0.5, 0.75, 0.9, 0.95]))\n",
    "\n",
    "    if \"similarity\" in integrated.columns:\n",
    "        print(\"\\nSimilarity:\")\n",
    "        print(integrated[\"similarity\"].describe(percentiles=[0.25, 0.5, 0.75, 0.9, 0.95]))\n",
    "\n",
    "    if \"score\" in integrated.columns:\n",
    "        print(\"\\nScore:\")\n",
    "        print(integrated[\"score\"].describe(percentiles=[0.25, 0.5, 0.75, 0.9, 0.95]))\n",
    "\n",
    "    if \"complaint_id\" in integrated.columns:\n",
    "        unique_complaints = integrated[\"complaint_id\"].nunique()\n",
    "        print(f\"\\nUnique complaints: {unique_complaints:,}\")\n",
    "        print(f\"Avg matches/complaint: {len(integrated) / max(1, unique_complaints):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a85a1b3",
   "metadata": {},
   "source": [
    "### Print Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c8883e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hybrid integration\n",
      "Complaint file: ../data/processed/philly_311_cleaned.csv\n",
      "Business file: ../data/processed/yelp_clustered_categories.json\n",
      "Output file: ../data/processed/final_integrated_dataset.csv\n",
      "Radius: 500.0m | Top-k: 3\n",
      "Weights: geo=0.6, text=0.4\n",
      "\n",
      "Stage 1: Load datasets\n",
      "Complaints: ../data/processed/philly_311_cleaned.csv\n",
      "Loaded complaints: 245,808\n",
      "Businesses: ../data/processed/yelp_clustered_categories.json\n",
      "Loaded businesses: 150,243\n",
      "\n",
      "Stage 2: Build spatial index\n",
      "Radius: 500.0m\n",
      "Grid cells: 22,360\n",
      "\n",
      "Stage 3: Tokenize\n",
      "Complaints: 245,808\n",
      "Complaint tokens: done\n",
      "Businesses: 150,243\n",
      "Business tokens: done\n",
      "\n",
      "Stage 4: Match\n",
      "Complaints to process: 245,808\n",
      "Progress: 50,000 / 245,808\n",
      "Progress: 100,000 / 245,808\n",
      "Progress: 150,000 / 245,808\n",
      "Progress: 200,000 / 245,808\n",
      "Matched complaints: 242,190\n",
      "Match records: 707,348\n",
      "\n",
      "Stage 5: Save output\n",
      "Saved: ../data/processed/final_integrated_dataset.csv\n",
      "\n",
      "Done\n",
      "\n",
      "Match summary\n",
      "Total matches: 707,348\n",
      "\n",
      "Distance (m):\n",
      "count    707348.000000\n",
      "mean        177.425175\n",
      "std         122.986278\n",
      "min           0.000000\n",
      "25%          74.990000\n",
      "50%         153.020000\n",
      "75%         262.010000\n",
      "90%         363.130000\n",
      "95%         415.760000\n",
      "max         500.000000\n",
      "Name: distance_m, dtype: float64\n",
      "\n",
      "Similarity:\n",
      "count    707348.000000\n",
      "mean          0.000631\n",
      "std           0.008527\n",
      "min           0.000000\n",
      "25%           0.000000\n",
      "50%           0.000000\n",
      "75%           0.000000\n",
      "90%           0.000000\n",
      "95%           0.000000\n",
      "max           0.200000\n",
      "Name: similarity, dtype: float64\n",
      "\n",
      "Score:\n",
      "count    707348.000000\n",
      "mean          0.387342\n",
      "std           0.147672\n",
      "min           0.000000\n",
      "25%           0.285800\n",
      "50%           0.416600\n",
      "75%           0.510300\n",
      "90%           0.559500\n",
      "95%           0.576500\n",
      "max           0.653500\n",
      "Name: score, dtype: float64\n",
      "\n",
      "Unique complaints: 242,190\n",
      "Avg matches/complaint: 2.92\n"
     ]
    }
   ],
   "source": [
    "cfg = IntegrationConfig()\n",
    "print(\"\\nHybrid integration\")\n",
    "print(f\"Complaint file: {cfg.complaint_path}\")\n",
    "print(f\"Business file: {cfg.business_path}\")\n",
    "print(f\"Output file: {cfg.output_path}\")\n",
    "print(f\"Radius: {cfg.radius_m}m | Top-k: {cfg.top_k}\")\n",
    "print(f\"Weights: geo={cfg.weight_geo}, text={cfg.weight_text}\")\n",
    "\n",
    "integrated = integrate_datasets(cfg)\n",
    "\n",
    "if not integrated.empty:\n",
    "    print(\"\\nStage 5: Save output\")\n",
    "    os.makedirs(os.path.dirname(cfg.output_path), exist_ok=True)\n",
    "    integrated.to_csv(cfg.output_path, index=False)\n",
    "    print(f\"Saved: {cfg.output_path}\")\n",
    "\n",
    "    print(\"\\nDone\")\n",
    "    summarize_matches(integrated)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS4630Proj1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
